{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bc14cee",
   "metadata": {},
   "source": [
    "# Proceso GLUE Eventos Amplitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789c3647",
   "metadata": {},
   "source": [
    "## 1. Cargamos las librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9c909a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e93f0a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import boto3\n",
    "import ast\n",
    "from datetime import datetime, timedelta\n",
    "import awswrangler as wr\n",
    "from itertools import chain\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "\n",
    "glue = boto3.client('glue')\n",
    "s3 = boto3.resource('s3')\n",
    "ssm = boto3.client('ssm') \n",
    "lakeformation = boto3.client('lakeformation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc5afe",
   "metadata": {},
   "source": [
    "### 2. Armamos el proceso en \"GLUE LTV-RFM AR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "455c696a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting get_data_amplitude.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile get_data_amplitude.py\n",
    "\n",
    "import sys\n",
    "import pyspark.sql.functions as func\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "import boto3\n",
    "import ast\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import gc\n",
    "import sys\n",
    "from pyspark.conf import SparkConf\n",
    "import pandas as pd\n",
    "\n",
    "print('Lectura de parámetros')\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "print('NOW:', datetime.now())\n",
    "\n",
    "args = getResolvedOptions(sys.argv,\n",
    "                          ['bucket_amplitude_data', \n",
    "                           'today', \n",
    "                           'kms_key_arn', \n",
    "                           'recommendations_bucket'])\n",
    "\n",
    "bucket_amplitude_data = args['bucket_amplitude_data']\n",
    "recommendations_bucket = args['recommendations_bucket']\n",
    "kms_key_id = args['kms_key_arn']\n",
    "today = args['today']\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print('Spark Configuración')\n",
    "\n",
    "spark_conf = SparkConf().setAll([\n",
    "  (\"spark.hadoop.fs.s3.enableServerSideEncryption\", \"true\"),\n",
    "  (\"spark.hadoop.fs.s3.serverSideEncryption.kms.keyId\", kms_key_id)\n",
    "])\n",
    "\n",
    "sc = SparkContext(conf=spark_conf)\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "logger = glueContext.get_logger()\n",
    "\n",
    "\n",
    "\n",
    "print('Crear objetos S3-ssm')\n",
    "# ----------------------------------------------------------------------------------\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "ssm = boto3.client('ssm')\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "print('Parámetros:')\n",
    "path_key_amplitude = 'ar/amplitude/tb_ar_amplitude_events_stage/'\n",
    "\n",
    "## FECHAS INTERVALO\n",
    "#print('1. CALCULO DE FECHAS')\n",
    "##Today llevado al primero del mes menos 1 día\n",
    "#today = datetime.strptime(today, '%Y-%m-%d').date().replace(day=1)\n",
    "#last_day=(today-pd.offsets.DateOffset(days=1)).date()\n",
    "##\n",
    "#first_day=(last_day-pd.offsets.DateOffset(days=365)).date()\n",
    "#\n",
    "#print('2. Intevalo de fechas analizada: ',first_day,'y',last_day)\n",
    "\n",
    "def first_and_last(today):\n",
    "    fecha=datetime.strptime(today, '%Y-%m-%d').date()\n",
    "    first_day=fecha.replace(day=1)\n",
    "    next_month = fecha.replace(day=28) + timedelta(days=4)\n",
    "    last_day_of_month = next_month - timedelta(days=next_month.day)\n",
    "    return first_day,last_day_of_month\n",
    "\n",
    "print('Declaración de funciones')\n",
    "def list_objects_function(buckets_, first_day, last_day, keys_, retrieve_last=False):\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(buckets_)\n",
    "    files_in_bucket = list(bucket.objects.all())\n",
    "    files_objets = [f\"s3://{buckets_}/\" + i.key for i in files_in_bucket if\n",
    "                        (i.key.find(keys_) >= 0) and (i.key.find('.parquet') >= 0)]\n",
    "    df_bucket_files = pd.DataFrame({\n",
    "            'key': [i[:(i.find('dt=') + 14)] for i in files_objets],\n",
    "            'path': files_objets,\n",
    "            'date': pd.to_datetime([i[(i.find('dt=') + 3):(i.find('dt=') + 13)] for i in files_objets])\n",
    "        })\n",
    "    files=list(df_bucket_files.loc[df_bucket_files['date'].between(str(first_day),str(last_day)),'path'].values)\n",
    "    return files\n",
    "\n",
    "\n",
    "\n",
    "map_events = {\n",
    "    \"cuoti_selecciona_elegircuotas\" : \"cuotificaciones\",\n",
    "    \"cuoti_sigue_seleccion_consumos\" : \"cuotificaciones\",\n",
    "    \"prestamos_selecciona_simular_prestamo\": \"prestamos\",\n",
    "    \"prestamos_espera\": \"prestamos\",\n",
    "    \"general_ingresa_promociones\": \"promociones\",\n",
    "    \"recargas_click_empezar\": \"recargas\",\n",
    "    \"recargas_click_repetir\": \"recargas\",\n",
    "    \"transferencia_selecciona_tieneuala\": \"transferencia_c2c\",\n",
    "    \"transferencia_selecciona_notieneuala\": \"transferencia_cvu\",\n",
    "    \"general_ingresa_cobros\": \"cobros\",\n",
    "    \"cobros_acepta_tyc\" : \"cobros\",\n",
    "    \"cobros_elige_link\": \"cobros\",\n",
    "    \"cobros_elige_mpos\": \"cobros\",\n",
    "    \"pagos_empezar\": \"pago_servicios\",\n",
    "    \"click_inversiones\":\"inversiones\"\n",
    "}\n",
    "\n",
    "eventos_recommendations = list(map_events.keys())\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "first_day,last_day = first_and_last(today)\n",
    "print('Primer dia',first_day)\n",
    "print('Ultimo dia',last_day)\n",
    "\n",
    "files_objets_amplitude = list_objects_function(bucket_amplitude_data, first_day, last_day ,path_key_amplitude)\n",
    "\n",
    "print(f'Hay {len(files_objets_amplitude)} archivos de survival en la carpeta')\n",
    "df_amplitude = spark.read.parquet(*files_objets_amplitude).select(['user_id',\"os_name\",\"event_type\",\"event_time\"])\n",
    "df_amplitude=df_amplitude.filter(df_amplitude.event_type.isin(eventos_recommendations))\n",
    "\n",
    "df_amplitude = df_amplitude.withColumn('year_month', F.date_format(df_amplitude.event_time,'YYYY-MM'))\n",
    "\n",
    "df_amplitude = df_amplitude.drop(\"event_time\")\n",
    "\n",
    "df_amplitude = df_amplitude.na.replace(map_events,1,\"event_type\")\n",
    "\n",
    "df_amplitude = (df_amplitude    \n",
    "      .groupBy(['user_id', 'event_type', 'year_month'])\n",
    "      .agg(F.count('event_type').alias('cant'),\n",
    "           F.max('os_name').alias('os_name'))\n",
    "      .groupBy(['user_id','year_month','os_name'])\n",
    "      .pivot(\"event_type\")\n",
    "      .agg(F.sum('cant'))\n",
    "      .na.fill(0)\n",
    "      )\n",
    "\n",
    "df_amplitude.write\\\n",
    "     .format('parquet')\\\n",
    "     .save(f's3://{recommendations_bucket}/data/raw/amplitude/dt={str(first_day)}', mode='overwrite')\n",
    "\n",
    "print('Ubicación files', f's3://{recommendations_bucket}/data/raw/amplitude/dt={str(first_day)}')\n",
    "\n",
    "#DELETE $FOLDER$\n",
    "\n",
    "def retrieve_files(path, file_type, list_dates):\n",
    "    bucket=path.split('/')[2]\n",
    "    prefix='/'.join(path.split('/')[3:])\n",
    "    list_objects=list(s3.Bucket(bucket).objects.all())\n",
    "    list_objects=[f's3://{bucket}/{i.key}' for i in list_objects if ((i.key.find(prefix)>=0) & any(x in i.key.lower() for x in list_dates) & (i.key.find(file_type)>=0))]\n",
    "    return list_objects\n",
    "\n",
    "\n",
    "delete_files = retrieve_files(path=f's3://{recommendations_bucket}/data/', file_type='$folder$', list_dates=['$folder$'])\n",
    "print('Files to delete', delete_files)\n",
    "files_keys=[]\n",
    "for i in range(0,len(delete_files)):\n",
    "    files_keys=files_keys+[{'Key':('/').join(delete_files[i].split('/')[3:])}]\n",
    "if len(files_keys)>0:\n",
    "    s3_client.delete_objects(Bucket=recommendations_bucket,\n",
    "                             Delete={'Objects':files_keys})\n",
    "del delete_files\n",
    "gc.collect()\n",
    "\n",
    "print(df_amplitude.show())\n",
    "print((df_amplitude.count(), len(df_amplitude.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f1fc87a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name='test-job_recommendations_amplitude'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a680bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrar job\n",
    "#glue.delete_job(\n",
    "#    JobName=job_name\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a5483",
   "metadata": {},
   "source": [
    "## 3. Generamos los parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aec0b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = '2021-01-10'\n",
    "bucket_amplitude_data='churn-ds-stage'  ## AFIP, GP, etc\n",
    "recommendations_bucket='test-uala-arg-datalake-aiml-recommendations'  # Para outputs\n",
    "kms_key_arn='arn:aws:kms:us-east-1:322149183112:key/9cc44b23-c5e9-46cb-9987-0982d21f8d00' ## key para desencriptar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "553fc0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".py uploaded\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Guardar el archivo .py\n",
    "s3.meta.client.upload_file('get_data_amplitude.py', \n",
    "                           recommendations_bucket, #bucket\n",
    "                           'artifacts/code/amplitude/get_data_amplitude.py' #key+filename\n",
    ")\n",
    "print('.py uploaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf56516a",
   "metadata": {},
   "source": [
    "## 4. Creamos el job de GLUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "081b607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#job = glue.create_job(Name=job_name, \n",
    "#                      GlueVersion='2.0',\n",
    "#                      Role='ML_AWSGlueServiceRole',\n",
    "#                      Command={'Name': 'glueetl',\n",
    "#                               'ScriptLocation': f's3://{recommendations_bucket}/artifacts/code/amplitude/get_data_amplitude.py'},\n",
    "#                      DefaultArguments={\n",
    "#                        '--additional-python-modules': 'dateutil==2.8.1'},\n",
    "#                      MaxCapacity=3\n",
    "#                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5171da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_run = glue.start_job_run(\n",
    "    JobName = job_name,\n",
    "    Arguments = {\n",
    "        '--today':today,\n",
    "        '--bucket_amplitude_data': bucket_amplitude_data,\n",
    "        '--recommendations_bucket': recommendations_bucket,\n",
    "        '--kms_key_arn': kms_key_arn\n",
    "    } \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "735e4d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JobRunId': 'jr_cf2978c1ed6c989cac356896b755adc916606f4449ff8bf16a36c86932e7beca', 'ResponseMetadata': {'RequestId': 'ded488e5-e827-4452-bb2d-5e053957841f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 20 May 2021 19:56:31 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '82', 'connection': 'keep-alive', 'x-amzn-requestid': 'ded488e5-e827-4452-bb2d-5e053957841f'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(job_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f7cb1fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "MAX_WAIT_TIME=time.time() + 60*10 # 1 hour\n",
    "max_time = time.time() + MAX_WAIT_TIME\n",
    "while time.time() < max_time:\n",
    "    response=glue.get_job_run(JobName=job_name, RunId=job_run['JobRunId'])\n",
    "    status = response['JobRun']['JobRunState']\n",
    "    print('Job run: {}'.format(status))\n",
    "    \n",
    "    if status == 'SUCCEEDED' or status == 'FAILED':\n",
    "        break\n",
    "        \n",
    "    time.sleep(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c898d21",
   "metadata": {},
   "source": [
    "## 5. Controlamos la carga de datos en el bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "66f24ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=wr.s3.read_parquet(f's3://test-uala-arg-datalake-aiml-recommendations/data/raw/amplitude',dataset=True,partition_filter=lambda x: '2021-01-01' in x[\"dt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "734ac887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>year_month</th>\n",
       "      <th>os_name</th>\n",
       "      <th>cobros</th>\n",
       "      <th>cuotificaciones</th>\n",
       "      <th>inversiones</th>\n",
       "      <th>pago_servicios</th>\n",
       "      <th>prestamos</th>\n",
       "      <th>promociones</th>\n",
       "      <th>recargas</th>\n",
       "      <th>transferencia_c2c</th>\n",
       "      <th>transferencia_cvu</th>\n",
       "      <th>dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00360d0c-2638-458b-a1b6-144cb3ba4b78</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>ios</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00509604-ce55-47db-93f6-6bca3867ef05</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>005e8359-5408-4b28-b2a0-a5ef6955f158</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0072cfda-2213-4b1a-b4bf-3969ccacde3f</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>android</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0074aa89-9148-4d94-9bee-e55bd591d038</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                user_id year_month  os_name  cobros  \\\n",
       "0  00360d0c-2638-458b-a1b6-144cb3ba4b78    2021-01      ios       0   \n",
       "1  00509604-ce55-47db-93f6-6bca3867ef05    2021-01  android       0   \n",
       "2  005e8359-5408-4b28-b2a0-a5ef6955f158    2021-01  android       0   \n",
       "3  0072cfda-2213-4b1a-b4bf-3969ccacde3f    2021-01  android       2   \n",
       "4  0074aa89-9148-4d94-9bee-e55bd591d038    2021-01  android       0   \n",
       "\n",
       "   cuotificaciones  inversiones  pago_servicios  prestamos  promociones  \\\n",
       "0                0            4               0          0            0   \n",
       "1                0            0               0          0            0   \n",
       "2                0            0               0          0            0   \n",
       "3                0            0               0          0            0   \n",
       "4                2           13               2          0            0   \n",
       "\n",
       "   recargas  transferencia_c2c  transferencia_cvu          dt  \n",
       "0         0                  4                  1  2021-01-01  \n",
       "1         0                 10                  4  2021-01-01  \n",
       "2         0                  2                  1  2021-01-01  \n",
       "3         4                  4                  1  2021-01-01  \n",
       "4         5                 10                  4  2021-01-01  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a2701f",
   "metadata": {},
   "source": [
    "## 6. Carga histórica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fe0274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_amplitude_data='churn-ds-stage'  ## AFIP, GP, etc\n",
    "recommendations_bucket='test-uala-arg-datalake-aiml-recommendations'  # Para outputs\n",
    "kms_key_arn='arn:aws:kms:us-east-1:322149183112:key/9cc44b23-c5e9-46cb-9987-0982d21f8d00' ## key para desencriptar\n",
    "list_fechas=['2020-05-01','2020-06-01','2020-07-01','2020-08-01','2020-09-01','2020-10-01','2020-11-01','2020-12-01',\n",
    "            '2021-01-01','2021-02-01','2021-03-01']\n",
    "job_name='test-job_recommendations_amplitude'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eb27120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: 2020-08\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: SUCCEEDED\n",
      "Procesando: 2020-09\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: SUCCEEDED\n",
      "Procesando: 2020-10\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: SUCCEEDED\n",
      "Procesando: 2020-11\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: SUCCEEDED\n",
      "Procesando: 2020-12\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: SUCCEEDED\n",
      "Procesando: 2021-01\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: SUCCEEDED\n",
      "Procesando: 2021-02\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: SUCCEEDED\n",
      "Procesando: 2021-03\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "#for value in list_fechas:\n",
    "    print(\"Procesando:\",value[:7])\n",
    "    job_run = glue.start_job_run(\n",
    "        JobName = job_name,\n",
    "        Arguments = {\n",
    "            '--today':value,\n",
    "            '--bucket_amplitude_data': bucket_amplitude_data,\n",
    "            '--recommendations_bucket': recommendations_bucket,\n",
    "            '--kms_key_arn': kms_key_arn\n",
    "        } \n",
    "    )\n",
    "    MAX_WAIT_TIME= 3600 # 1 hour\n",
    "    max_time = time.time() + MAX_WAIT_TIME\n",
    "    while time.time() < max_time:\n",
    "        response=glue.get_job_run(JobName=job_name, RunId=job_run['JobRunId'])\n",
    "        status = response['JobRun']['JobRunState']\n",
    "        print('Job run: {}'.format(status))\n",
    "\n",
    "        if status == 'SUCCEEDED':\n",
    "            time.sleep(60)\n",
    "            break\n",
    "        elif status == 'FAILED':\n",
    "            print (\"Error para fecha:\",value[:7],\" \\n\" )\n",
    "            sys.exit(1)\n",
    "        time.sleep(45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
