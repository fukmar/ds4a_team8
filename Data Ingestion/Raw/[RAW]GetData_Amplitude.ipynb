{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed48da5",
   "metadata": {},
   "source": [
    "# Proceso GLUE Eventos Amplitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f169e",
   "metadata": {},
   "source": [
    "## 1. Cargamos las librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a445ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# TIPOS DE TRANSACCIONES\n",
    "print('3. PARAMETRIA DE TRANSACCIONES')\n",
    "plan_user_to_user = '18-C2C LOCAL'\n",
    "commerce_automatic_debit = ['00300008', '00300009', '00300007', '00300000', '00300001', '00300006', '00300002',\n",
    "                            '00300005', '00300004']\n",
    "plan_automatic_debit = ['15-DEBITO AUTOMATI', '16-DEBITO AUTOMATI']\n",
    "commerce_telerecargas = ['00300000']\n",
    "commerce_investments_withdraw = ['00300009']\n",
    "commerce_bill_payment = ['00300001']\n",
    "commerce_cash_out_cvu = ['00300007']\n",
    "plan_withdraw_atm = '3-ADELANTOS EN PES'\n",
    "commerce_loan_installment = ['00300002']\n",
    "\n",
    "\n",
    "transaction_type_rfm=['USER_TO_USER','WITHDRAW_ATM','AUTOMATIC_DEBIT','TELERECARGAS_CARGA','CONSUMPTION_POS',\n",
    "                      'LOAN_INSTALLMENT_PAYMENT','BILL_PAYMENT', 'CASH_OUT_CVU','INVESTMENTS_DEPOSIT']\n",
    "\n",
    "#transaction_type_rfm=['USER_TO_USER','AUTOMATIC_DEBIT','TELERECARGAS_CARGA','CONSUMPTION_POS','BILL_PAYMENT',\n",
    "#                      'CASH_OUT_CVU','WITHDRAW_ATM','INVESTMENTS_DEPOSIT']\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------- OBTENER LAS COTIZACIONES DE DOLAR  ------------------------------------\n",
    "print('NOW:',datetime.now())\n",
    "\n",
    "#----------------------------- COTIZACIONES --------------------------------------\n",
    "print('4. IMPORTAR EL ARCHIVO DE COTIZACIONES, TRANSFORMAR')\n",
    "\n",
    "cotizaciones_key= 'parameters'\n",
    "cotizaciones_fn = 'cotizaciones_stage.csv'\n",
    "cotizaciones_path = f's3://{recommendations_bucket}/{cotizaciones_key}/{cotizaciones_fn}'\n",
    "coti = pd.read_csv(cotizaciones_path, usecols=[1,5])\n",
    "coti['fecha'] = pd.to_datetime(coti['fecha'])\n",
    "coti['fecha_mes'] = coti['fecha'].dt.to_period('M')\n",
    "coti_gb = coti.groupby('fecha_mes', as_index=False).agg({'venta_uala':'max'})\n",
    "df_cotizaciones = coti.merge(coti_gb, left_on='fecha_mes', \n",
    "                             right_on='fecha_mes', \n",
    "                             how='left', suffixes=('', '_max_mes'))[['fecha','venta_uala_max_mes']]\n",
    "\n",
    "path_csv = f's3://{recommendations_bucket}/{cotizaciones_key}/cotizacion_mensual.csv'\n",
    "df_cotizaciones.to_csv(path_or_buf=path_csv)\n",
    "    \n",
    "# IMPORTAR EL ARCHIVO DE LAS COTIZACIONES \n",
    "valor_cotizacion= (spark.read\n",
    "                   .option('header',True)\n",
    "                   .option(\"inferSchema\",\"true\")\n",
    "                   .csv(path_csv)\n",
    "                  )\n",
    "\n",
    "cot_spark = valor_cotizacion.select(['fecha', 'venta_uala_max_mes'])\n",
    "cot_spark = (cot_spark\n",
    "             .withColumn('fecha', F.to_date(cot_spark.fecha,\"yyyy-MM-dd\"))\n",
    "             .withColumnRenamed('fecha', 'fecha_venta')\n",
    "             .withColumn('venta_uala_max_mes', cot_spark.venta_uala_max_mes.cast('double'))\n",
    "            )\n",
    "\n",
    "#----------------------------- SURVIVAL ACCOUNTS ------------------------------------\n",
    "# Ir a buscar accounts que usa survival para usar las mismas\n",
    "print('5. OBTENER SURVIVAL ACCOUNTS')\n",
    "keys_surv_accounts='data/raw/users'\n",
    "\n",
    "files_objets_surv, ultima_fecha_surv = list_objects_function(last_day,first_day,buckets_ = survival_bucket,keys_ = keys_surv_accounts, retrieve_last=True)\n",
    "\n",
    "print(f'Hay {len(files_objets_surv)} archivos de survival en la carpeta más reciente datada en {ultima_fecha_surv}')\n",
    "df_survival = spark.read.parquet(*files_objets_surv).select(['accountgp'])\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# LEER ARCHIVOS 7001\n",
    "print('6. LEER ARCHIVOS 7001')\n",
    "\n",
    "columns = ['fechaautorizacion','cuenta', 'plan', 'comercio', 'moneda', 'estado','importe','reversa']\n",
    "files7001=list_objects_function(last_day=last_day,first_day=first_day,buckets_=bucket_amplitude_data,keys_=path_key_amplitude)\n",
    "\n",
    "# to read parquet file\n",
    "print('La cantidad de parquets 7001 es', len(files7001))\n",
    "\n",
    "df = spark.read.parquet(*files7001).select(columns)\n",
    "print('La cantidad de filas es:',df.count(),'y la cantidad columnas', len(df.columns))\n",
    "\n",
    "\n",
    "#----------------------------- TRANSFORMACIONES 7001 --------------------------------------\n",
    "print('7. TRANSFORMACIONES 7001')\n",
    "\n",
    "# CONVERTIR EN FORMATO FECHA\n",
    "print('7.1. FORMATO FECHA')\n",
    "df=df.withColumn(\"fecha\",F.substring(df['fechaautorizacion'], 1, 8))\n",
    "df=df.drop('fechaautorizacion')\n",
    "#original\n",
    "#df=df.withColumn('fecha',F.to_date(df.fecha,\"ddmmyyyy\"))\n",
    "#propuesta\n",
    "#df=df.withColumn('fecha',F.unix_timestamp(df.fecha,\"ddMMyyyy\"))\n",
    "df=df.withColumn('fecha',F.to_date(df.fecha,\"ddMMyyyy\"))\n",
    "\n",
    "\n",
    "# FORMATO CUENTA, IMPORTE\n",
    "print('7.2. FORMATO IMPORTE')\n",
    "\n",
    "df = (df\n",
    "        .withColumn(\"cuenta\", df[\"cuenta\"].cast('integer'))\n",
    "        .withColumn('importe', F.regexp_replace('importe', '-', ''))\n",
    "     )\n",
    "df = df.withColumn(\"importe\", df[\"importe\"].cast('double'))\n",
    "\n",
    "\n",
    "###  FILTER SURVIVAL !!!  ####\n",
    "print('7.3. INNER JOIN SURVIVAL')\n",
    "df_survival = df_survival.withColumn(\"accountgp\", df_survival[\"accountgp\"].cast('integer'))\n",
    "df_survival = df_survival.distinct()\n",
    "#df_survival = df_survival.dropDuplicates()\n",
    "df = df.join(df_survival, df.cuenta == df_survival.accountgp, how = 'inner')\n",
    "########\n",
    "\n",
    "\n",
    "# ELIMINO LAS TRANSACCIONES CANCELADAS\n",
    "print('7.4. ELIMINO LAS TRANSACCIONES CANCELADAS DEL ARCHIVO 7001')\n",
    "\n",
    "df = df.where((df.estado == 'APROBADA') & (~df.plan.isin(['5-DEVOLUCION PESOS', '6-DEVOLUCION DOLAR'])))\n",
    "df = df.withColumn(\"cuenta\", df[\"cuenta\"].cast('integer'))\n",
    "print('La cantidad de filas es:',df.count(),'y la cantidad columnas', len(df.columns))\n",
    "\n",
    "\n",
    "\n",
    "### JOIN PARA TRAER COTIZACIONES\n",
    "print('7.5. JOIN PARA TRAER COTIZACIONES')\n",
    "\n",
    "df = df.join(cot_spark, df.fecha == cot_spark.fecha_venta, how = 'left')\n",
    "print('Cantidad de fechas sin cotizacion: ',df.where(F.col('venta_uala_max_mes').isNull()).select('venta_uala_max_mes').count())\n",
    "\n",
    "# FORMATO Y FILTRADO IMPORTE \n",
    "print('7.6. FORMATO, FILTRADO, IMPORTE Y CONVERSION USD')\n",
    "\n",
    "df=(\n",
    "    df\n",
    "        .withColumn(\"importe\", df[\"importe\"]/100)\n",
    "    # FORMAT: REVERSA\n",
    "        .withColumn(\"reversa\", df[\"reversa\"].cast('integer'))\n",
    "    # CONVERTIR EL IMPORTE SI ES UNA EN REVERSA\n",
    "        .withColumn('importe', F.when(F.col('reversa') == 1, \n",
    "                                      (F.col('importe')*-1)).otherwise(F.col('importe')))\n",
    "    # PASAR A DOLAR\n",
    "        .withColumn('importe_dolar', F.when(F.col('moneda') == '002', \n",
    "                                            (F.col('importe'))).otherwise(F.col('importe')/F.col('venta_uala_max_mes')))\n",
    ")        \n",
    "\n",
    "# FILTRO DE TRX CON IMPORTE DOLAR > 1 \n",
    "df = df.filter(df['importe_dolar'] > 1)\n",
    "\n",
    "\n",
    "# ELIMINAR COLUMNAS\n",
    "df=df.drop('importe','reversa', 'fecha_venta')\n",
    "\n",
    "\n",
    "# DEFINIR UNA VARIABLE TRANSACTION_TYPE\n",
    "print('7.7 7001 DEFINIR EL TRANSCTIONTYPE')\n",
    "\n",
    "\n",
    "df=df.withColumn(\"transaction_type\", F.lit('CONSUMPTION_POS'))\n",
    "\n",
    "# USER TO USER\n",
    "df=df.withColumn('transaction_type', \n",
    "                    F.when(F.col('plan') == plan_user_to_user, \n",
    "                          'USER_TO_USER').otherwise(df['transaction_type']))\n",
    "\n",
    "# AUTOMATIC DEBIT\n",
    "df=df.withColumn('transaction_type', \n",
    "                     F.when((~F.col('comercio').isin(plan_automatic_debit)) & \n",
    "                          ( F.col('plan').isin(plan_automatic_debit)), \n",
    "                          'AUTOMATIC_DEBIT').otherwise(df['transaction_type']))\n",
    "\n",
    "# TELERECARGAS CARGA\n",
    "df=df.withColumn('transaction_type', \n",
    "                     F.when(F.col('comercio').isin(commerce_telerecargas), \n",
    "                          'TELERECARGAS_CARGA').otherwise(df['transaction_type']))\n",
    "\n",
    "# INVESTMENTS WITHDRAW\n",
    "df=df.withColumn('transaction_type', \n",
    "                     F.when(F.col('comercio').isin(commerce_investments_withdraw), \n",
    "                          'INVESTMENTS_WITHDRAW').otherwise(df['transaction_type']))\n",
    "\n",
    "# CASH OUT CVU\n",
    "df=df.withColumn('transaction_type', \n",
    "                     F.when(F.col('comercio').isin(commerce_cash_out_cvu), \n",
    "                          'CASH_OUT_CVU').otherwise(df['transaction_type']))\n",
    "\n",
    "# WITHDRAW ATM\n",
    "df=df.withColumn('transaction_type', \n",
    "                     F.when(F.col('plan').isin(plan_withdraw_atm), \n",
    "                          'WITHDRAW_ATM').otherwise(df['transaction_type']))\n",
    "\n",
    "# BILL PAYMENTS\n",
    "df=df.withColumn('transaction_type', \n",
    "                     F.when(F.col('comercio').isin(commerce_bill_payment), \n",
    "                          'BILL_PAYMENT').otherwise(df['transaction_type']))\n",
    "\n",
    "# LOAN INSTALLMENT PAYMENT\n",
    "df=df.withColumn('transaction_type', \n",
    "                     F.when(F.col('comercio').isin(commerce_loan_installment), \n",
    "                          'LOAN_INSTALLMENT_PAYMENT').otherwise(df['transaction_type']))\n",
    "\n",
    "\n",
    "# FILTRAR POR LAS TRANSACCIONES\n",
    "df=df.filter(df.transaction_type.isin(transaction_type_rfm))\n",
    "\n",
    "\n",
    "fecha_min = df.agg({\"fecha\": \"min\"}).collect()[0]\n",
    "fecha_max = df.agg({\"fecha\": \"max\"}).collect()[0]\n",
    "print(f'Luego de todos los filtros hay {df.count()} filas')\n",
    "print(f'Fecha más vieja es {fecha_min}')\n",
    "print(f'Fecha más nueva es {fecha_max}')\n",
    "\n",
    "\n",
    "\n",
    "#########################\n",
    "#     Groupby total     #\n",
    "#########################\n",
    "print('8. GROUP BY TOTAL')\n",
    "\n",
    "\n",
    "df = (df    \n",
    "      .groupBy(['cuenta', 'fecha'])\n",
    "      .agg(F.sum('importe_dolar').alias('importe_dolar')\n",
    "     ))\n",
    "\n",
    "base_ =(df\n",
    "        .groupBy('cuenta')\n",
    "        .agg(F.expr('count(distinct fecha)-1').alias('frequency'),\n",
    "            F.datediff(F.max('fecha'), F.min('fecha')).alias('recency'),\n",
    "            F.datediff(F.lit(last_day), F.min('fecha')).alias('T'),\n",
    "            F.mean('importe_dolar').alias('importe_dolar')\n",
    "            ))\n",
    "\n",
    "\n",
    "base_ = base_.filter(base_['frequency'] > 0)\n",
    "\n",
    "\n",
    "#########################\n",
    "# Separar cal y holdout #\n",
    "#########################\n",
    "print('9. SEPARACION CAL Y HOLDOUT')\n",
    "\n",
    "### CAL ###  <last_day-30\n",
    "print('9.1 CAL')\n",
    "\n",
    "\n",
    "cal_date=(last_day-pd.offsets.DateOffset(days=30)).date()\n",
    "\n",
    "print(f'cal date es {cal_date}')\n",
    "\n",
    "df_cal = df.where(F.col('fecha') < cal_date)\n",
    "\n",
    "base_cal=(df_cal\n",
    "   .groupBy('cuenta')\n",
    "   .agg(F.expr('count(distinct fecha)-1').alias('frequency_cal'),\n",
    "        F.datediff(F.max('fecha'), F.min('fecha')).alias('recency_cal'),\n",
    "        F.datediff(F.lit(last_day), F.min('fecha')).alias('T_cal'),\n",
    "        F.mean('importe_dolar').alias('importe_dolar_cal')\n",
    "       ))\n",
    "\n",
    "\n",
    "### HOLDOUT ### >=last_day-30\n",
    "print('9.2 HOLDOUT')\n",
    "\n",
    "df_holdout = df.filter(F.col('fecha').between(cal_date, last_day))\n",
    "\n",
    "base_holdout=(df_holdout\n",
    "       .groupBy('cuenta')\n",
    "       .agg(F.expr('count(distinct fecha)-1').alias('frequency_holdout'),\n",
    "            F.datediff(F.max('fecha'), F.min('fecha')).alias('recency_holdout'),\n",
    "            F.datediff(F.lit(last_day), F.min('fecha')).alias('T_holdout'),\n",
    "            F.mean('importe_dolar').alias('importe_dolar_holdout')\n",
    "       ))\n",
    "\n",
    "base_holdout = base_holdout.withColumnRenamed('cuenta', 'cuenta_holdout')\n",
    "\n",
    "####### JOIN holdout-cal #######\n",
    "print('10. JOIN HOLDOUT-CAL')\n",
    "\n",
    "\n",
    "base_train = base_cal.join(base_holdout, base_cal.cuenta == base_holdout.cuenta_holdout, how = 'outer')\n",
    "base_train = base_train.withColumn('duration_holdout', F.lit('30'))\n",
    "base_train = base_train.withColumn('cuenta', F.coalesce(base_train.cuenta, base_train.cuenta_holdout)) \n",
    "base_train = base_train.drop('cuenta_holdout')\n",
    "base_train = base_train.withColumn('duration_holdout', F.lit('30'))\n",
    "\n",
    "base_train = base_train.withColumn('frequency_cal', F.coalesce(base_train.frequency_cal, F.lit(\"0\") )) \n",
    "base_train = base_train.filter(base_train['frequency_cal'] > 0)\n",
    "\n",
    "################################\n",
    "print('11. ESCRIBIR PARQUET BASE Y BASE_TRAIN')\n",
    "\n",
    "base_.write\\\n",
    "     .format('parquet')\\\n",
    "     .save(f's3://{recommendations_bucket}/data/dt={str(last_day)}/full', mode='overwrite')\n",
    "\n",
    "base_train.write\\\n",
    "     .format('parquet')\\\n",
    "     .save(f's3://{recommendations_bucket}/data/dt={str(last_day)}/train', mode='overwrite')\n",
    "\n",
    "\n",
    "\n",
    "print('Ubicación files', f's3://{recommendations_bucket}/data/dt={str(last_day)}')\n",
    "print('Eliminar archivos vacíos de Spark')\n",
    "print('Función retrieve files')\n",
    "\n",
    "def retrieve_files(path, file_type, list_dates):\n",
    "    bucket=path.split('/')[2]\n",
    "    prefix='/'.join(path.split('/')[3:])\n",
    "    list_objects=list(s3.Bucket(bucket).objects.all())\n",
    "    list_objects=[f's3://{bucket}/{i.key}' for i in list_objects if ((i.key.find(prefix)>=0) & any(x in i.key.lower() for x in list_dates) & (i.key.find(file_type)>=0))]\n",
    "    return list_objects\n",
    "\n",
    "\n",
    "\n",
    "delete_files = retrieve_files(path=f's3://{recommendations_bucket}/data/', file_type='$folder$', list_dates=['$folder$'])\n",
    "print('Files to delete', delete_files)\n",
    "files_keys=[]\n",
    "for i in range(0,len(delete_files)):\n",
    "    files_keys=files_keys+[{'Key':('/').join(delete_files[i].split('/')[3:])}]\n",
    "if len(files_keys)>0:\n",
    "    s3_client.delete_objects(Bucket=recommendations_bucket,\n",
    "                             Delete={'Objects':files_keys})\n",
    "del delete_files\n",
    "gc.collect()\n",
    "\n",
    "                              \n",
    "print('NOW:', datetime.now())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2853d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9134060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import boto3\n",
    "import ast\n",
    "from datetime import datetime, timedelta\n",
    "import awswrangler as wr\n",
    "from itertools import chain\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "\n",
    "glue = boto3.client('glue')\n",
    "s3 = boto3.resource('s3')\n",
    "ssm = boto3.client('ssm') \n",
    "lakeformation = boto3.client('lakeformation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0727ef3b",
   "metadata": {},
   "source": [
    "### 2. Armamos el proceso en \"GLUE LTV-RFM AR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "482d56fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting get_data_amplitude.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile get_data_amplitude.py\n",
    "\n",
    "import sys\n",
    "import pyspark.sql.functions as func\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "import boto3\n",
    "import ast\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import gc\n",
    "import sys\n",
    "from pyspark.conf import SparkConf\n",
    "import pandas as pd\n",
    "\n",
    "print('Lectura de parámetros')\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "print('NOW:', datetime.now())\n",
    "\n",
    "args = getResolvedOptions(sys.argv,\n",
    "                          ['bucket_amplitude_data', \n",
    "                           'today', \n",
    "                           'kms_key_arn', \n",
    "                           'recommendations_bucket'])\n",
    "\n",
    "bucket_amplitude_data = args['bucket_amplitude_data']\n",
    "recommendations_bucket = args['recommendations_bucket']\n",
    "kms_key_id = args['kms_key_arn']\n",
    "today = args['today']\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print('Spark Configuración')\n",
    "\n",
    "spark_conf = SparkConf().setAll([\n",
    "  (\"spark.hadoop.fs.s3.enableServerSideEncryption\", \"true\"),\n",
    "  (\"spark.hadoop.fs.s3.serverSideEncryption.kms.keyId\", kms_key_id)\n",
    "])\n",
    "\n",
    "sc = SparkContext(conf=spark_conf)\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "logger = glueContext.get_logger()\n",
    "\n",
    "\n",
    "\n",
    "print('Crear objetos S3-ssm')\n",
    "# ----------------------------------------------------------------------------------\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "ssm = boto3.client('ssm')\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "print('Parámetros:')\n",
    "path_key_amplitude = 'ar/amplitude/tb_ar_amplitude_events_stage/'\n",
    "\n",
    "## FECHAS INTERVALO\n",
    "#print('1. CALCULO DE FECHAS')\n",
    "##Today llevado al primero del mes menos 1 día\n",
    "#today = datetime.strptime(today, '%Y-%m-%d').date().replace(day=1)\n",
    "#last_day=(today-pd.offsets.DateOffset(days=1)).date()\n",
    "##\n",
    "#first_day=(last_day-pd.offsets.DateOffset(days=365)).date()\n",
    "#\n",
    "#print('2. Intevalo de fechas analizada: ',first_day,'y',last_day)\n",
    "\n",
    "def first_and_last(today):\n",
    "    fecha=datetime.strptime(today, '%Y-%m-%d').date()\n",
    "    first_day=fecha.replace(day=1)\n",
    "    next_month = fecha.replace(day=28) + timedelta(days=4)\n",
    "    last_day_of_month = next_month - timedelta(days=next_month.day)\n",
    "    return first_day,last_day_of_month\n",
    "\n",
    "print('Declaración de funciones')\n",
    "def list_objects_function(buckets_, first_day, last_day, keys_, retrieve_last=False):\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(buckets_)\n",
    "    files_in_bucket = list(bucket.objects.all())\n",
    "    files_objets = [f\"s3://{buckets_}/\" + i.key for i in files_in_bucket if\n",
    "                        (i.key.find(keys_) >= 0) and (i.key.find('.parquet') >= 0)]\n",
    "    df_bucket_files = pd.DataFrame({\n",
    "            'key': [i[:(i.find('dt=') + 14)] for i in files_objets],\n",
    "            'path': files_objets,\n",
    "            'date': pd.to_datetime([i[(i.find('dt=') + 3):(i.find('dt=') + 13)] for i in files_objets])\n",
    "        })\n",
    "    files=list(df_bucket_files.loc[df_bucket_files['date'].between(str(first_day),str(last_day)),'path'].values)\n",
    "    return files\n",
    "\n",
    "\n",
    "\n",
    "map_events = {\n",
    "    \"cuoti_selecciona_elegircuotas\" : \"cuotificaciones\",\n",
    "    \"cuoti_sigue_seleccion_consumos\" : \"cuotificaciones\",\n",
    "    \"prestamos_selecciona_simular_prestamo\": \"prestamos\",\n",
    "    \"prestamos_espera\": \"prestamos\",\n",
    "    \"general_ingresa_promociones\": \"promociones\",\n",
    "    \"recargas_click_empezar\": \"recargas\",\n",
    "    \"recargas_click_repetir\": \"recargas\",\n",
    "    \"transferencia_selecciona_tieneuala\": \"transferencia_c2c\",\n",
    "    \"transferencia_selecciona_notieneuala\": \"transferencia_cvu\",\n",
    "    \"general_ingresa_cobros\": \"cobros\",\n",
    "    \"cobros_acepta_tyc\" : \"cobros\",\n",
    "    \"cobros_elige_link\": \"cobros\",\n",
    "    \"cobros_elige_mpos\": \"cobros\",\n",
    "    \"pagos_empezar\": \"pago_servicios\",\n",
    "    \"click_inversiones\":\"inversiones\"\n",
    "}\n",
    "\n",
    "eventos_recommendations = list(map_events.keys())\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "first_day,last_day = first_and_last(today)\n",
    "print('Primer dia',first_day)\n",
    "print('Ultimo dia',last_day)\n",
    "\n",
    "files_objets_amplitude = list_objects_function(bucket_amplitude_data, first_day, last_day ,path_key_amplitude)\n",
    "\n",
    "print(f'Hay {len(files_objets_amplitude)} archivos de survival en la carpeta')\n",
    "df_amplitude = spark.read.parquet(*files_objets_amplitude).select(['user_id',\"os_name\",\"event_type\",\"event_time\"])\n",
    "df_amplitude=df_amplitude.filter(df_amplitude.event_type.isin(eventos_recommendations))\n",
    "\n",
    "df_amplitude = df_amplitude.withColumn('year_month', F.date_format(df_amplitude.event_time,'YYYY-MM'))\n",
    "\n",
    "df_amplitude = df_amplitude.drop(\"event_time\")\n",
    "\n",
    "df_amplitude = df_amplitude.na.replace(map_events,1,\"event_type\")\n",
    "\n",
    "df_amplitude = (df_amplitude    \n",
    "      .groupBy(['user_id', 'event_type', 'year_month'])\n",
    "      .agg(F.count('event_type').alias('cant'),\n",
    "           F.max('os_name').alias('os_name'))\n",
    "      .groupBy(['user_id','year_month','os_name'])\n",
    "      .pivot(\"event_type\")\n",
    "      .agg(F.sum('cant'))\n",
    "      .na.fill(0)\n",
    "      )\n",
    "\n",
    "df_amplitude.write\\\n",
    "     .format('parquet')\\\n",
    "     .save(f's3://{recommendations_bucket}/data/raw/amplitude/dt={str(first_day)}', mode='overwrite')\n",
    "\n",
    "print('Ubicación files', f's3://{recommendations_bucket}/data/raw/amplitude/dt={str(first_day)}')\n",
    "\n",
    "#DELETE $FOLDER$\n",
    "\n",
    "def retrieve_files(path, file_type, list_dates):\n",
    "    bucket=path.split('/')[2]\n",
    "    prefix='/'.join(path.split('/')[3:])\n",
    "    list_objects=list(s3.Bucket(bucket).objects.all())\n",
    "    list_objects=[f's3://{bucket}/{i.key}' for i in list_objects if ((i.key.find(prefix)>=0) & any(x in i.key.lower() for x in list_dates) & (i.key.find(file_type)>=0))]\n",
    "    return list_objects\n",
    "\n",
    "\n",
    "delete_files = retrieve_files(path=f's3://{recommendations_bucket}/data/', file_type='$folder$', list_dates=['$folder$'])\n",
    "print('Files to delete', delete_files)\n",
    "files_keys=[]\n",
    "for i in range(0,len(delete_files)):\n",
    "    files_keys=files_keys+[{'Key':('/').join(delete_files[i].split('/')[3:])}]\n",
    "if len(files_keys)>0:\n",
    "    s3_client.delete_objects(Bucket=recommendations_bucket,\n",
    "                             Delete={'Objects':files_keys})\n",
    "del delete_files\n",
    "gc.collect()\n",
    "\n",
    "print(df_amplitude.show())\n",
    "print((df_amplitude.count(), len(df_amplitude.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "893901fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name='test-job_recommendations_amplitude'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6d85a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrar job\n",
    "#glue.delete_job(\n",
    "#    JobName=job_name\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e39117",
   "metadata": {},
   "source": [
    "## 3. Generamos los parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "606ce828",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = '2021-01-10'\n",
    "bucket_amplitude_data='churn-ds-stage'  ## AFIP, GP, etc\n",
    "recommendations_bucket='test-uala-arg-datalake-aiml-recommendations'  # Para outputs\n",
    "kms_key_arn='arn:aws:kms:us-east-1:322149183112:key/9cc44b23-c5e9-46cb-9987-0982d21f8d00' ## key para desencriptar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "80505646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".py uploaded\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Guardar el archivo .py\n",
    "s3.meta.client.upload_file('get_data_amplitude.py', \n",
    "                           recommendations_bucket, #bucket\n",
    "                           'artifacts/code/amplitude/get_data_amplitude.py' #key+filename\n",
    ")\n",
    "print('.py uploaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf81022",
   "metadata": {},
   "source": [
    "## 4. Creamos el job de GLUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b98efb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#job = glue.create_job(Name=job_name, \n",
    "#                      GlueVersion='2.0',\n",
    "#                      Role='ML_AWSGlueServiceRole',\n",
    "#                      Command={'Name': 'glueetl',\n",
    "#                               'ScriptLocation': f's3://{recommendations_bucket}/artifacts/code/amplitude/get_data_amplitude.py'},\n",
    "#                      DefaultArguments={\n",
    "#                        '--additional-python-modules': 'dateutil==2.8.1'},\n",
    "#                      MaxCapacity=3\n",
    "#                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ac0df99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_run = glue.start_job_run(\n",
    "    JobName = job_name,\n",
    "    Arguments = {\n",
    "        '--today':today,\n",
    "        '--bucket_amplitude_data': bucket_amplitude_data,\n",
    "        '--recommendations_bucket': recommendations_bucket,\n",
    "        '--kms_key_arn': kms_key_arn\n",
    "    } \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7b4409b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JobRunId': 'jr_cf2978c1ed6c989cac356896b755adc916606f4449ff8bf16a36c86932e7beca', 'ResponseMetadata': {'RequestId': 'ded488e5-e827-4452-bb2d-5e053957841f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 20 May 2021 19:56:31 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '82', 'connection': 'keep-alive', 'x-amzn-requestid': 'ded488e5-e827-4452-bb2d-5e053957841f'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(job_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8eecc376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "MAX_WAIT_TIME=time.time() + 60*10 # 1 hour\n",
    "max_time = time.time() + MAX_WAIT_TIME\n",
    "while time.time() < max_time:\n",
    "    response=glue.get_job_run(JobName=job_name, RunId=job_run['JobRunId'])\n",
    "    status = response['JobRun']['JobRunState']\n",
    "    print('Job run: {}'.format(status))\n",
    "    \n",
    "    if status == 'SUCCEEDED' or status == 'FAILED':\n",
    "        break\n",
    "        \n",
    "    time.sleep(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb2c5a8",
   "metadata": {},
   "source": [
    "## 5. Controlamos la carga de datos en el bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "977fe329",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=wr.s3.read_parquet(f's3://test-uala-arg-datalake-aiml-recommendations/data/raw/amplitude',dataset=True,partition_filter=lambda x: '2021-01-01' in x[\"dt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c4d7d01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>year_month</th>\n",
       "      <th>os_name</th>\n",
       "      <th>cobros</th>\n",
       "      <th>cuotificaciones</th>\n",
       "      <th>inversiones</th>\n",
       "      <th>pago_servicios</th>\n",
       "      <th>prestamos</th>\n",
       "      <th>promociones</th>\n",
       "      <th>recargas</th>\n",
       "      <th>transferencia_c2c</th>\n",
       "      <th>transferencia_cvu</th>\n",
       "      <th>dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00360d0c-2638-458b-a1b6-144cb3ba4b78</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>ios</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00509604-ce55-47db-93f6-6bca3867ef05</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>005e8359-5408-4b28-b2a0-a5ef6955f158</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0072cfda-2213-4b1a-b4bf-3969ccacde3f</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>android</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0074aa89-9148-4d94-9bee-e55bd591d038</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                user_id year_month  os_name  cobros  \\\n",
       "0  00360d0c-2638-458b-a1b6-144cb3ba4b78    2021-01      ios       0   \n",
       "1  00509604-ce55-47db-93f6-6bca3867ef05    2021-01  android       0   \n",
       "2  005e8359-5408-4b28-b2a0-a5ef6955f158    2021-01  android       0   \n",
       "3  0072cfda-2213-4b1a-b4bf-3969ccacde3f    2021-01  android       2   \n",
       "4  0074aa89-9148-4d94-9bee-e55bd591d038    2021-01  android       0   \n",
       "\n",
       "   cuotificaciones  inversiones  pago_servicios  prestamos  promociones  \\\n",
       "0                0            4               0          0            0   \n",
       "1                0            0               0          0            0   \n",
       "2                0            0               0          0            0   \n",
       "3                0            0               0          0            0   \n",
       "4                2           13               2          0            0   \n",
       "\n",
       "   recargas  transferencia_c2c  transferencia_cvu          dt  \n",
       "0         0                  4                  1  2021-01-01  \n",
       "1         0                 10                  4  2021-01-01  \n",
       "2         0                  2                  1  2021-01-01  \n",
       "3         4                  4                  1  2021-01-01  \n",
       "4         5                 10                  4  2021-01-01  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca1d5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
