{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed48da5",
   "metadata": {},
   "source": [
    "# Proceso GLUE Eventos Amplitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f169e",
   "metadata": {},
   "source": [
    "## 1. Cargamos las librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a445ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# TIPOS DE TRANSACCIONES\n",
    "print('3. PARAMETRIA DE TRANSACCIONES')\n",
    "plan_user_to_user = '18-C2C LOCAL'\n",
    "commerce_automatic_debit = ['00300008', '00300009', '00300007', '00300000', '00300001', '00300006', '00300002',\n",
    "                            '00300005', '00300004']\n",
    "plan_automatic_debit = ['15-DEBITO AUTOMATI', '16-DEBITO AUTOMATI']\n",
    "commerce_telerecargas = ['00300000']\n",
    "commerce_investments_withdraw = ['00300009']\n",
    "commerce_bill_payment = ['00300001']\n",
    "commerce_cash_out_cvu = ['00300007']\n",
    "plan_withdraw_atm = '3-ADELANTOS EN PES'\n",
    "commerce_loan_installment = ['00300002']\n",
    "\n",
    "\n",
    "transaction_type_rfm=['USER_TO_USER','WITHDRAW_ATM','AUTOMATIC_DEBIT','TELERECARGAS_CARGA','CONSUMPTION_POS',\n",
    "                      'LOAN_INSTALLMENT_PAYMENT','BILL_PAYMENT', 'CASH_OUT_CVU','INVESTMENTS_DEPOSIT']\n",
    "\n",
    "#transaction_type_rfm=['USER_TO_USER','AUTOMATIC_DEBIT','TELERECARGAS_CARGA','CONSUMPTION_POS','BILL_PAYMENT',\n",
    "#                      'CASH_OUT_CVU','WITHDRAW_ATM','INVESTMENTS_DEPOSIT']\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------- OBTENER LAS COTIZACIONES DE DOLAR  ------------------------------------\n",
    "print('NOW:',datetime.now())\n",
    "\n",
    "#----------------------------- COTIZACIONES --------------------------------------\n",
    "print('4. IMPORTAR EL ARCHIVO DE COTIZACIONES, TRANSFORMAR')\n",
    "\n",
    "cotizaciones_key= 'parameters'\n",
    "cotizaciones_fn = 'cotizaciones_stage.csv'\n",
    "cotizaciones_path = f's3://{recommendations_bucket}/{cotizaciones_key}/{cotizaciones_fn}'\n",
    "coti = pd.read_csv(cotizaciones_path, usecols=[1,5])\n",
    "coti['fecha'] = pd.to_datetime(coti['fecha'])\n",
    "coti['fecha_mes'] = coti['fecha'].dt.to_period('M')\n",
    "coti_gb = coti.groupby('fecha_mes', as_index=False).agg({'venta_uala':'max'})\n",
    "df_cotizaciones = coti.merge(coti_gb, left_on='fecha_mes', \n",
    "                             right_on='fecha_mes', \n",
    "                             how='left', suffixes=('', '_max_mes'))[['fecha','venta_uala_max_mes']]\n",
    "\n",
    "path_csv = f's3://{recommendations_bucket}/{cotizaciones_key}/cotizacion_mensual.csv'\n",
    "df_cotizaciones.to_csv(path_or_buf=path_csv)\n",
    "    \n",
    "# IMPORTAR EL ARCHIVO DE LAS COTIZACIONES \n",
    "valor_cotizacion= (spark.read\n",
    "                   .option('header',True)\n",
    "                   .option(\"inferSchema\",\"true\")\n",
    "                   .csv(path_csv)\n",
    "                  )\n",
    "\n",
    "cot_spark = valor_cotizacion.select(['fecha', 'venta_uala_max_mes'])\n",
    "cot_spark = (cot_spark\n",
    "             .withColumn('fecha', F.to_date(cot_spark.fecha,\"yyyy-MM-dd\"))\n",
    "             .withColumnRenamed('fecha', 'fecha_venta')\n",
    "             .withColumn('venta_uala_max_mes', cot_spark.venta_uala_max_mes.cast('double'))\n",
    "            )\n",
    "\n",
    "#----------------------------- SURVIVAL ACCOUNTS ------------------------------------\n",
    "# Ir a buscar accounts que usa survival para usar las mismas\n",
    "print('5. OBTENER SURVIVAL ACCOUNTS')\n",
    "keys_surv_accounts='data/raw/users'\n",
    "\n",
    "files_objets_surv, ultima_fecha_surv = list_objects_function(last_day,first_day,buckets_ = survival_bucket,keys_ = keys_surv_accounts, retrieve_last=True)\n",
    "\n",
    "print(f'Hay {len(files_objets_surv)} archivos de survival en la carpeta mÃ¡s reciente datada en {ultima_fecha_surv}')\n",
    "df_survival = spark.read.parquet(*files_objets_surv).select(['accountgp'])\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# LEER ARCHIVOS 7001\n",
    "print('6. LEER ARCHIVOS 7001')\n",
    "\n",
    "columns = ['fechaautorizacion','cuenta', 'plan', 'comercio', 'moneda', 'estado','importe','reversa']\n",
    "files7001=list_objects_function(last_day=last_day,first_day=first_day,buckets_=bucket_amplitude_data,keys_=path_key_amplitude)\n",
    "\n",
    "# to read parquet file\n",
    "print('La cantidad de parquets 7001 es', len(files7001))\n",
    "\n",
    "df = spark.read.parquet(*files7001).select(columns)\n",
    "print('La cantidad de filas es:',df.count(),'y la cantidad columnas', len(df.columns))\n",
    "\n",
    "\n",
    "#----------------------------- TRANSFORMACIONES 7001 --------------------------------------\n",
    "print('7. TRANSFORMACIONES 7001')\n",
    "\n",
    "# CONVERTIR EN FORMATO FECHA\n",
    "print('7.1. FORMATO FECHA')\n",
    "df=df.withColumn(\"fecha\",F.substring(df['fechaautorizacion'], 1, 8))\n",
    "df=df.drop('fechaautorizacion')\n",
    "#original\n",
    "#df=df.withColumn('fecha',F.to_date(df.fecha,\"ddmmyyyy\"))\n",
    "#propuesta\n",
    "#df=df.withColumn('fecha',F.unix_timestamp(df.fecha,\"ddMMyyyy\"))\n",
    "df=df.withColumn('fecha',F.to_date(df.fecha,\"ddMMyyyy\"))\n",
    "\n",
    "\n",
    "# FORMATO CUENTA, IMPORTE\n",
    "print('7.2. FORMATO IMPORTE')\n",
    "\n",
    "df = (df\n",
    "        .withColumn(\"cuenta\", df[\"cuenta\"].cast('integer'))\n",
    "        .withColumn('importe', F.regexp_replace('importe', '-', ''))\n",
    "     )\n",
    "df = df.withColumn(\"importe\", df[\"importe\"].cast('double'))\n",
    "\n",
    "\n",
    "###  FILTER SURVIVAL !!!  ####\n",
    "print('7.3. INNER JOIN SURVIVAL')\n",
    "df_survival = df_survival.withColumn(\"accountgp\", df_survival[\"accountgp\"].cast('integer'))\n",
    "df_survival = df_survival.distinct()\n",
    "#df_survival = df_survival.dropDuplicates()\n",
    "df = df.join(df_survival, df.cuenta == df_survival.accountgp, how = 'inner')\n",
    "########\n",
    "\n",
    "\n",
    "# ELIMINO LAS TRANSACCIONES CANCELADAS\n",
    "print('7.4. ELIMINO LAS TRANSACCIONES CANCELADAS DEL ARCHIVO 7001')\n",
    "\n",
    "df = df.where((df.estado == 'APROBADA') & (~df.plan.isin(['5-DEVOLUCION PESOS', '6-DEVOLUCION DOLAR'])))\n",
    "df = df.withColumn(\"cuenta\", df[\"cuenta\"].cast('integer'))\n",
    "print('La cantidad de filas es:',df.count(),'y la cantidad columnas', len(df.columns))\n",
    "\n",
    "\n",
    "\n",
    "### JOIN PARA TRAER COTIZACIONES\n",
    "print('7.5. JOIN PARA TRAER COTIZACIONES')\n",
    "\n",
    "df = df.join(cot_spark, df.fecha == cot_spark.fecha_venta, how = 'left')\n",
    "print('Cantidad de fechas sin cotizacion: ',df.where(F.col('venta_uala_max_mes').isNull()).select('venta_uala_max_mes').count())\n",
    "\n",
    "# FORMATO Y FILTRADO IMPORTE \n",
    "print('7.6. FORMATO, FILTRADO, IMPORTE Y CONVERSION USD')\n",
    "\n",
    "df=(\n",
    "    df\n",
    "        .withColumn(\"importe\", df[\"importe\"]/100)\n",
    "    # FORMAT: REVERSA\n",
    "        .withColumn(\"reversa\", df[\"reversa\"].cast('integer'))\n",
    "    # CONVERTIR EL IMPORTE SI ES UNA EN REVERSA\n",
    "        .withColumn('importe', F.when(F.col('reversa') == 1, \n",
    "                                      (F.col('importe')*-1)).otherwise(F.col('importe')))\n",
    "    # PASAR A DOLAR\n",
    "        .withColumn('importe_dolar', F.when(F.col('moneda') == '002', \n",
    "                                            (F.col('importe'))).otherwise(F.col('importe')/F.col('venta_uala_max_mes')))\n",
    ")        \n",
    "\n",
    "# FILTRO DE TRX CON IMPORTE DOLAR > 1 \n",
    "df = df.filter(df['importe_dolar'] > 1)\n",
    "\n",
    "\n",
    "# ELIMINAR COLUMNAS\n",
    "df=df.drop('importe','reversa', 'fecha_venta')\n",
    "\n",
    "\n",
    "# DEFINIR UNA VARIABLE TRANSACTION_TYPE\n",
    "print('7.7 7001 DEFINIR EL TRANSCTIONTYPE')\n",
    "\n",
    "\n",
    "df=df.withColumn(\"transaction_type\", F.lit('CONSUMPTION_POS'))\n",
    "\n",
    "# USER TO USER\n",
    "df=df.withColumn('transaction_type', \n",
    "                    F.when(F.col('plan') == plan_user_to_user, \n",
    "                          'USER_TO_USER').otherwise(df['transaction_type']))\n",
    "\n",
    "# AUTOMATIC DEBIT\n",
    "df=df.withColumn('transaction_type', \n",
    "                     F.when((~F.col('comercio').isin(plan_automatic_debit)) & \n",
    "                          ( F.col('plan').isin(plan_automatic_debit)), \n",
    "                          'AUTOMATIC_DEBIT').otherwise(df['transaction_type']))\n",
    "\n",
    "# TELERECARGAS CARGA\n",
    "df=df.withColumn('transaction_type', \n",
    "                     F.when(F.col('comercio').isin(commerce_telerecargas), \n",
    "                          'TELERECARGAS_CARGA').otherwise(df['transaction_type']))\n",
    "\n",
    "# INVESTMENTS WITHDRAW\n",
    "df=df.withColumn('transaction_type', \n",
    "                     F.when(F.col('comercio').isin(commerce_investments_withdraw), \n",
    "                          'INVESTMENTS_WITHDRAW').otherwise(df['transaction_type']))\n",
    "\n",
    "# CASH OUT CVU\n",
    "df=df.withColumn('transaction_type', \n",
    "                     F.when(F.col('comercio').isin(commerce_cash_out_cvu), \n",
    "                          'CASH_OUT_CVU').otherwise(df['transaction_type']))\n",
    "\n",
    "# WITHDRAW ATM\n",
    "df=df.withColumn('transaction_type', \n",
    "                     F.when(F.col('plan').isin(plan_withdraw_atm), \n",
    "                          'WITHDRAW_ATM').otherwise(df['transaction_type']))\n",
    "\n",
    "# BILL PAYMENTS\n",
    "df=df.withColumn('transaction_type', \n",
    "                     F.when(F.col('comercio').isin(commerce_bill_payment), \n",
    "                          'BILL_PAYMENT').otherwise(df['transaction_type']))\n",
    "\n",
    "# LOAN INSTALLMENT PAYMENT\n",
    "df=df.withColumn('transaction_type', \n",
    "                     F.when(F.col('comercio').isin(commerce_loan_installment), \n",
    "                          'LOAN_INSTALLMENT_PAYMENT').otherwise(df['transaction_type']))\n",
    "\n",
    "\n",
    "# FILTRAR POR LAS TRANSACCIONES\n",
    "df=df.filter(df.transaction_type.isin(transaction_type_rfm))\n",
    "\n",
    "\n",
    "fecha_min = df.agg({\"fecha\": \"min\"}).collect()[0]\n",
    "fecha_max = df.agg({\"fecha\": \"max\"}).collect()[0]\n",
    "print(f'Luego de todos los filtros hay {df.count()} filas')\n",
    "print(f'Fecha mÃ¡s vieja es {fecha_min}')\n",
    "print(f'Fecha mÃ¡s nueva es {fecha_max}')\n",
    "\n",
    "\n",
    "\n",
    "#########################\n",
    "#     Groupby total     #\n",
    "#########################\n",
    "print('8. GROUP BY TOTAL')\n",
    "\n",
    "\n",
    "df = (df    \n",
    "      .groupBy(['cuenta', 'fecha'])\n",
    "      .agg(F.sum('importe_dolar').alias('importe_dolar')\n",
    "     ))\n",
    "\n",
    "base_ =(df\n",
    "        .groupBy('cuenta')\n",
    "        .agg(F.expr('count(distinct fecha)-1').alias('frequency'),\n",
    "            F.datediff(F.max('fecha'), F.min('fecha')).alias('recency'),\n",
    "            F.datediff(F.lit(last_day), F.min('fecha')).alias('T'),\n",
    "            F.mean('importe_dolar').alias('importe_dolar')\n",
    "            ))\n",
    "\n",
    "\n",
    "base_ = base_.filter(base_['frequency'] > 0)\n",
    "\n",
    "\n",
    "#########################\n",
    "# Separar cal y holdout #\n",
    "#########################\n",
    "print('9. SEPARACION CAL Y HOLDOUT')\n",
    "\n",
    "### CAL ###  <last_day-30\n",
    "print('9.1 CAL')\n",
    "\n",
    "\n",
    "cal_date=(last_day-pd.offsets.DateOffset(days=30)).date()\n",
    "\n",
    "print(f'cal date es {cal_date}')\n",
    "\n",
    "df_cal = df.where(F.col('fecha') < cal_date)\n",
    "\n",
    "base_cal=(df_cal\n",
    "   .groupBy('cuenta')\n",
    "   .agg(F.expr('count(distinct fecha)-1').alias('frequency_cal'),\n",
    "        F.datediff(F.max('fecha'), F.min('fecha')).alias('recency_cal'),\n",
    "        F.datediff(F.lit(last_day), F.min('fecha')).alias('T_cal'),\n",
    "        F.mean('importe_dolar').alias('importe_dolar_cal')\n",
    "       ))\n",
    "\n",
    "\n",
    "### HOLDOUT ### >=last_day-30\n",
    "print('9.2 HOLDOUT')\n",
    "\n",
    "df_holdout = df.filter(F.col('fecha').between(cal_date, last_day))\n",
    "\n",
    "base_holdout=(df_holdout\n",
    "       .groupBy('cuenta')\n",
    "       .agg(F.expr('count(distinct fecha)-1').alias('frequency_holdout'),\n",
    "            F.datediff(F.max('fecha'), F.min('fecha')).alias('recency_holdout'),\n",
    "            F.datediff(F.lit(last_day), F.min('fecha')).alias('T_holdout'),\n",
    "            F.mean('importe_dolar').alias('importe_dolar_holdout')\n",
    "       ))\n",
    "\n",
    "base_holdout = base_holdout.withColumnRenamed('cuenta', 'cuenta_holdout')\n",
    "\n",
    "####### JOIN holdout-cal #######\n",
    "print('10. JOIN HOLDOUT-CAL')\n",
    "\n",
    "\n",
    "base_train = base_cal.join(base_holdout, base_cal.cuenta == base_holdout.cuenta_holdout, how = 'outer')\n",
    "base_train = base_train.withColumn('duration_holdout', F.lit('30'))\n",
    "base_train = base_train.withColumn('cuenta', F.coalesce(base_train.cuenta, base_train.cuenta_holdout)) \n",
    "base_train = base_train.drop('cuenta_holdout')\n",
    "base_train = base_train.withColumn('duration_holdout', F.lit('30'))\n",
    "\n",
    "base_train = base_train.withColumn('frequency_cal', F.coalesce(base_train.frequency_cal, F.lit(\"0\") )) \n",
    "base_train = base_train.filter(base_train['frequency_cal'] > 0)\n",
    "\n",
    "################################\n",
    "print('11. ESCRIBIR PARQUET BASE Y BASE_TRAIN')\n",
    "\n",
    "base_.write\\\n",
    "     .format('parquet')\\\n",
    "     .save(f's3://{recommendations_bucket}/data/dt={str(last_day)}/full', mode='overwrite')\n",
    "\n",
    "base_train.write\\\n",
    "     .format('parquet')\\\n",
    "     .save(f's3://{recommendations_bucket}/data/dt={str(last_day)}/train', mode='overwrite')\n",
    "\n",
    "\n",
    "\n",
    "print('UbicaciÃ³n files', f's3://{recommendations_bucket}/data/dt={str(last_day)}')\n",
    "print('Eliminar archivos vacÃ­os de Spark')\n",
    "print('FunciÃ³n retrieve files')\n",
    "\n",
    "def retrieve_files(path, file_type, list_dates):\n",
    "    bucket=path.split('/')[2]\n",
    "    prefix='/'.join(path.split('/')[3:])\n",
    "    list_objects=list(s3.Bucket(bucket).objects.all())\n",
    "    list_objects=[f's3://{bucket}/{i.key}' for i in list_objects if ((i.key.find(prefix)>=0) & any(x in i.key.lower() for x in list_dates) & (i.key.find(file_type)>=0))]\n",
    "    return list_objects\n",
    "\n",
    "\n",
    "\n",
    "delete_files = retrieve_files(path=f's3://{recommendations_bucket}/data/', file_type='$folder$', list_dates=['$folder$'])\n",
    "print('Files to delete', delete_files)\n",
    "files_keys=[]\n",
    "for i in range(0,len(delete_files)):\n",
    "    files_keys=files_keys+[{'Key':('/').join(delete_files[i].split('/')[3:])}]\n",
    "if len(files_keys)>0:\n",
    "    s3_client.delete_objects(Bucket=recommendations_bucket,\n",
    "                             Delete={'Objects':files_keys})\n",
    "del delete_files\n",
    "gc.collect()\n",
    "\n",
    "                              \n",
    "print('NOW:', datetime.now())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2853d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9134060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import boto3\n",
    "import ast\n",
    "from datetime import datetime, timedelta\n",
    "import awswrangler as wr\n",
    "from itertools import chain\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "\n",
    "glue = boto3.client('glue')\n",
    "s3 = boto3.resource('s3')\n",
    "ssm = boto3.client('ssm') \n",
    "lakeformation = boto3.client('lakeformation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0727ef3b",
   "metadata": {},
   "source": [
    "### 2. Armamos el proceso en \"GLUE LTV-RFM AR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "482d56fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting get_data_amplitude.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile get_data_amplitude.py\n",
    "\n",
    "import sys\n",
    "import pyspark.sql.functions as func\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "import boto3\n",
    "import ast\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import gc\n",
    "import sys\n",
    "from pyspark.conf import SparkConf\n",
    "import pandas as pd\n",
    "\n",
    "print('Lectura de parÃ¡metros')\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "print('NOW:', datetime.now())\n",
    "\n",
    "args = getResolvedOptions(sys.argv,\n",
    "                          ['bucket_amplitude_data', \n",
    "                           'today', \n",
    "                           'kms_key_arn', \n",
    "                           'recommendations_bucket'])\n",
    "\n",
    "bucket_amplitude_data = args['bucket_amplitude_data']\n",
    "recommendations_bucket = args['recommendations_bucket']\n",
    "kms_key_id = args['kms_key_arn']\n",
    "today = args['today']\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print('Spark ConfiguraciÃ³n')\n",
    "\n",
    "spark_conf = SparkConf().setAll([\n",
    "  (\"spark.hadoop.fs.s3.enableServerSideEncryption\", \"true\"),\n",
    "  (\"spark.hadoop.fs.s3.serverSideEncryption.kms.keyId\", kms_key_id)\n",
    "])\n",
    "\n",
    "sc = SparkContext(conf=spark_conf)\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "logger = glueContext.get_logger()\n",
    "\n",
    "\n",
    "\n",
    "print('Crear objetos S3-ssm')\n",
    "# ----------------------------------------------------------------------------------\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "ssm = boto3.client('ssm')\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "print('ParÃ¡metros:')\n",
    "path_key_amplitude = 'ar/amplitude/tb_ar_amplitude_events_stage/'\n",
    "\n",
    "## FECHAS INTERVALO\n",
    "#print('1. CALCULO DE FECHAS')\n",
    "##Today llevado al primero del mes menos 1 dÃ­a\n",
    "#today = datetime.strptime(today, '%Y-%m-%d').date().replace(day=1)\n",
    "#last_day=(today-pd.offsets.DateOffset(days=1)).date()\n",
    "##\n",
    "#first_day=(last_day-pd.offsets.DateOffset(days=365)).date()\n",
    "#\n",
    "#print('2. Intevalo de fechas analizada: ',first_day,'y',last_day)\n",
    "\n",
    "def first_and_last(today):\n",
    "    fecha=datetime.strptime(today, '%Y-%m-%d').date()\n",
    "    first_day=fecha.replace(day=1)\n",
    "    next_month = fecha.replace(day=28) + timedelta(days=4)\n",
    "    last_day_of_month = next_month - timedelta(days=next_month.day)\n",
    "    return first_day,last_day_of_month\n",
    "\n",
    "print('DeclaraciÃ³n de funciones')\n",
    "def list_objects_function(buckets_, first_day, last_day, keys_, retrieve_last=False):\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(buckets_)\n",
    "    files_in_bucket = list(bucket.objects.all())\n",
    "    files_objets = [f\"s3://{buckets_}/\" + i.key for i in files_in_bucket if\n",
    "                        (i.key.find(keys_) >= 0) and (i.key.find('.parquet') >= 0)]\n",
    "    df_bucket_files = pd.DataFrame({\n",
    "            'key': [i[:(i.find('dt=') + 14)] for i in files_objets],\n",
    "            'path': files_objets,\n",
    "            'date': pd.to_datetime([i[(i.find('dt=') + 3):(i.find('dt=') + 13)] for i in files_objets])\n",
    "        })\n",
    "    files=list(df_bucket_files.loc[df_bucket_files['date'].between(str(first_day),str(last_day)),'path'].values)\n",
    "    return files\n",
    "\n",
    "\n",
    "\n",
    "map_events = {\n",
    "    \"cuoti_selecciona_elegircuotas\" : \"cuotificaciones\",\n",
    "    \"cuoti_sigue_seleccion_consumos\" : \"cuotificaciones\",\n",
    "    \"prestamos_selecciona_simular_prestamo\": \"prestamos\",\n",
    "    \"prestamos_espera\": \"prestamos\",\n",
    "    \"general_ingresa_promociones\": \"promociones\",\n",
    "    \"recargas_click_empezar\": \"recargas\",\n",
    "    \"recargas_click_repetir\": \"recargas\",\n",
    "    \"transferencia_selecciona_tieneuala\": \"transferencia_c2c\",\n",
    "    \"transferencia_selecciona_notieneuala\": \"transferencia_cvu\",\n",
    "    \"general_ingresa_cobros\": \"cobros\",\n",
    "    \"cobros_acepta_tyc\" : \"cobros\",\n",
    "    \"cobros_elige_link\": \"cobros\",\n",
    "    \"cobros_elige_mpos\": \"cobros\",\n",
    "    \"pagos_empezar\": \"pago_servicios\",\n",
    "    \"click_inversiones\":\"inversiones\"\n",
    "}\n",
    "\n",
    "eventos_recommendations = list(map_events.keys())\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "first_day,last_day = first_and_last(today)\n",
    "print('Primer dia',first_day)\n",
    "print('Ultimo dia',last_day)\n",
    "\n",
    "files_objets_amplitude = list_objects_function(bucket_amplitude_data, first_day, last_day ,path_key_amplitude)\n",
    "\n",
    "print(f'Hay {len(files_objets_amplitude)} archivos de survival en la carpeta')\n",
    "df_amplitude = spark.read.parquet(*files_objets_amplitude).select(['user_id',\"os_name\",\"event_type\",\"event_time\"])\n",
    "df_amplitude=df_amplitude.filter(df_amplitude.event_type.isin(eventos_recommendations))\n",
    "\n",
    "df_amplitude = df_amplitude.withColumn('year_month', F.date_format(df_amplitude.event_time,'YYYY-MM'))\n",
    "\n",
    "df_amplitude = df_amplitude.drop(\"event_time\")\n",
    "\n",
    "df_amplitude = df_amplitude.na.replace(map_events,1,\"event_type\")\n",
    "\n",
    "df_amplitude = (df_amplitude    \n",
    "      .groupBy(['user_id', 'event_type', 'year_month'])\n",
    "      .agg(F.count('event_type').alias('cant'),\n",
    "           F.max('os_name').alias('os_name'))\n",
    "      .groupBy(['user_id','year_month','os_name'])\n",
    "      .pivot(\"event_type\")\n",
    "      .agg(F.sum('cant'))\n",
    "      .na.fill(0)\n",
    "      )\n",
    "\n",
    "df_amplitude.write\\\n",
    "     .format('parquet')\\\n",
    "     .save(f's3://{recommendations_bucket}/data/raw/amplitude/dt={str(first_day)}', mode='overwrite')\n",
    "\n",
    "print('UbicaciÃ³n files', f's3://{recommendations_bucket}/data/raw/amplitude/dt={str(first_day)}')\n",
    "\n",
    "#DELETE $FOLDER$\n",
    "\n",
    "def retrieve_files(path, file_type, list_dates):\n",
    "    bucket=path.split('/')[2]\n",
    "    prefix='/'.join(path.split('/')[3:])\n",
    "    list_objects=list(s3.Bucket(bucket).objects.all())\n",
    "    list_objects=[f's3://{bucket}/{i.key}' for i in list_objects if ((i.key.find(prefix)>=0) & any(x in i.key.lower() for x in list_dates) & (i.key.find(file_type)>=0))]\n",
    "    return list_objects\n",
    "\n",
    "\n",
    "delete_files = retrieve_files(path=f's3://{recommendations_bucket}/data/', file_type='$folder$', list_dates=['$folder$'])\n",
    "print('Files to delete', delete_files)\n",
    "files_keys=[]\n",
    "for i in range(0,len(delete_files)):\n",
    "    files_keys=files_keys+[{'Key':('/').join(delete_files[i].split('/')[3:])}]\n",
    "if len(files_keys)>0:\n",
    "    s3_client.delete_objects(Bucket=recommendations_bucket,\n",
    "                             Delete={'Objects':files_keys})\n",
    "del delete_files\n",
    "gc.collect()\n",
    "\n",
    "print(df_amplitude.show())\n",
    "print((df_amplitude.count(), len(df_amplitude.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "893901fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name='test-job_recommendations_amplitude'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6d85a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrar job\n",
    "#glue.delete_job(\n",
    "#    JobName=job_name\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e39117",
   "metadata": {},
   "source": [
    "## 3. Generamos los parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "606ce828",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = '2021-01-10'\n",
    "bucket_amplitude_data='churn-ds-stage'  ## AFIP, GP, etc\n",
    "recommendations_bucket='test-uala-arg-datalake-aiml-recommendations'  # Para outputs\n",
    "kms_key_arn='arn:aws:kms:us-east-1:322149183112:key/9cc44b23-c5e9-46cb-9987-0982d21f8d00' ## key para desencriptar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "80505646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".py uploaded\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Guardar el archivo .py\n",
    "s3.meta.client.upload_file('get_data_amplitude.py', \n",
    "                           recommendations_bucket, #bucket\n",
    "                           'artifacts/code/amplitude/get_data_amplitude.py' #key+filename\n",
    ")\n",
    "print('.py uploaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf81022",
   "metadata": {},
   "source": [
    "## 4. Creamos el job de GLUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b98efb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#job = glue.create_job(Name=job_name, \n",
    "#                      GlueVersion='2.0',\n",
    "#                      Role='ML_AWSGlueServiceRole',\n",
    "#                      Command={'Name': 'glueetl',\n",
    "#                               'ScriptLocation': f's3://{recommendations_bucket}/artifacts/code/amplitude/get_data_amplitude.py'},\n",
    "#                      DefaultArguments={\n",
    "#                        '--additional-python-modules': 'dateutil==2.8.1'},\n",
    "#                      MaxCapacity=3\n",
    "#                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ac0df99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_run = glue.start_job_run(\n",
    "    JobName = job_name,\n",
    "    Arguments = {\n",
    "        '--today':today,\n",
    "        '--bucket_amplitude_data': bucket_amplitude_data,\n",
    "        '--recommendations_bucket': recommendations_bucket,\n",
    "        '--kms_key_arn': kms_key_arn\n",
    "    } \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7b4409b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JobRunId': 'jr_cf2978c1ed6c989cac356896b755adc916606f4449ff8bf16a36c86932e7beca', 'ResponseMetadata': {'RequestId': 'ded488e5-e827-4452-bb2d-5e053957841f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 20 May 2021 19:56:31 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '82', 'connection': 'keep-alive', 'x-amzn-requestid': 'ded488e5-e827-4452-bb2d-5e053957841f'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(job_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8eecc376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "MAX_WAIT_TIME=time.time() + 60*10 # 1 hour\n",
    "max_time = time.time() + MAX_WAIT_TIME\n",
    "while time.time() < max_time:\n",
    "    response=glue.get_job_run(JobName=job_name, RunId=job_run['JobRunId'])\n",
    "    status = response['JobRun']['JobRunState']\n",
    "    print('Job run: {}'.format(status))\n",
    "    \n",
    "    if status == 'SUCCEEDED' or status == 'FAILED':\n",
    "        break\n",
    "        \n",
    "    time.sleep(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb2c5a8",
   "metadata": {},
   "source": [
    "## 5. Controlamos la carga de datos en el bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "977fe329",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=wr.s3.read_parquet(f's3://test-uala-arg-datalake-aiml-recommendations/data/raw/amplitude',dataset=True,partition_filter=lambda x: '2021-01-01' in x[\"dt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c4d7d01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>year_month</th>\n",
       "      <th>os_name</th>\n",
       "      <th>cobros</th>\n",
       "      <th>cuotificaciones</th>\n",
       "      <th>inversiones</th>\n",
       "      <th>pago_servicios</th>\n",
       "      <th>prestamos</th>\n",
       "      <th>promociones</th>\n",
       "      <th>recargas</th>\n",
       "      <th>transferencia_c2c</th>\n",
       "      <th>transferencia_cvu</th>\n",
       "      <th>dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00360d0c-2638-458b-a1b6-144cb3ba4b78</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>ios</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00509604-ce55-47db-93f6-6bca3867ef05</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>005e8359-5408-4b28-b2a0-a5ef6955f158</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0072cfda-2213-4b1a-b4bf-3969ccacde3f</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>android</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0074aa89-9148-4d94-9bee-e55bd591d038</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                user_id year_month  os_name  cobros  \\\n",
       "0  00360d0c-2638-458b-a1b6-144cb3ba4b78    2021-01      ios       0   \n",
       "1  00509604-ce55-47db-93f6-6bca3867ef05    2021-01  android       0   \n",
       "2  005e8359-5408-4b28-b2a0-a5ef6955f158    2021-01  android       0   \n",
       "3  0072cfda-2213-4b1a-b4bf-3969ccacde3f    2021-01  android       2   \n",
       "4  0074aa89-9148-4d94-9bee-e55bd591d038    2021-01  android       0   \n",
       "\n",
       "   cuotificaciones  inversiones  pago_servicios  prestamos  promociones  \\\n",
       "0                0            4               0          0            0   \n",
       "1                0            0               0          0            0   \n",
       "2                0            0               0          0            0   \n",
       "3                0            0               0          0            0   \n",
       "4                2           13               2          0            0   \n",
       "\n",
       "   recargas  transferencia_c2c  transferencia_cvu          dt  \n",
       "0         0                  4                  1  2021-01-01  \n",
       "1         0                 10                  4  2021-01-01  \n",
       "2         0                  2                  1  2021-01-01  \n",
       "3         4                  4                  1  2021-01-01  \n",
       "4         5                 10                  4  2021-01-01  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca1d5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
