{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bfa7c24",
   "metadata": {},
   "source": [
    "# Proceso GLUE Merge Amplitude - 3022 - users - 7001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90148ede",
   "metadata": {},
   "source": [
    "## 1. Cargamos las librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00bc4e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71beda06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import boto3\n",
    "import ast\n",
    "from datetime import datetime, timedelta\n",
    "import awswrangler as wr\n",
    "from itertools import chain\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "glue = boto3.client('glue')\n",
    "s3 = boto3.resource('s3')\n",
    "ssm = boto3.client('ssm') \n",
    "lakeformation = boto3.client('lakeformation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abb152b",
   "metadata": {},
   "source": [
    "### 2. Armamos el proceso de glue para hacer el merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6cd3eece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting merge_stg.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile merge_stg.py\n",
    "\n",
    "import sys\n",
    "import pyspark.sql.functions as func\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "import boto3\n",
    "import ast\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import gc\n",
    "import sys\n",
    "from pyspark.conf import SparkConf\n",
    "import pandas as pd\n",
    "\n",
    "print('Lectura de parámetros')\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "print('NOW:', datetime.now())\n",
    "\n",
    "args = getResolvedOptions(sys.argv,\n",
    "                          ['today', \n",
    "                           'kms_key_arn', \n",
    "                           'recommendations_bucket'])\n",
    "\n",
    "recommendations_bucket = args['recommendations_bucket']\n",
    "kms_key_id = args['kms_key_arn']\n",
    "today = args['today']\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print('Spark Configuración')\n",
    "\n",
    "spark_conf = SparkConf().setAll([\n",
    "  (\"spark.hadoop.fs.s3.enableServerSideEncryption\", \"true\"),\n",
    "  (\"spark.hadoop.fs.s3.serverSideEncryption.kms.keyId\", kms_key_id)\n",
    "])\n",
    "\n",
    "sc = SparkContext(conf=spark_conf)\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "logger = glueContext.get_logger()\n",
    "\n",
    "\n",
    "\n",
    "print('Crear objetos S3-ssm')\n",
    "# ----------------------------------------------------------------------------------\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "ssm = boto3.client('ssm')\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "print('Parámetros:')\n",
    "path_key_survival_stg = 'data/raw/transactions/'\n",
    "path_key_amplitude = 'data/raw/amplitude/'\n",
    "path_key_cards = 'data/raw/cards/'\n",
    "\n",
    "#s3://uala-arg-datalake-aiml-survival-dev/data/monthly_stage/\n",
    "## FECHAS INTERVALO\n",
    "#print('1. CALCULO DE FECHAS')\n",
    "##Today llevado al primero del mes menos 1 día\n",
    "#today = datetime.strptime(today, '%Y-%m-%d').date().replace(day=1)\n",
    "#last_day=(today-pd.offsets.DateOffset(days=1)).date()\n",
    "##\n",
    "#first_day=(last_day-pd.offsets.DateOffset(days=365)).date()\n",
    "#\n",
    "#print('2. Intevalo de fechas analizada: ',first_day,'y',last_day)\n",
    "\n",
    "def first_and_last(today):\n",
    "    fecha=datetime.strptime(today, '%Y-%m-%d').date()\n",
    "    first_day=fecha.replace(day=1)\n",
    "    next_month = fecha.replace(day=28) + timedelta(days=4)\n",
    "    last_day_of_month = next_month - timedelta(days=next_month.day)\n",
    "    return first_day,last_day_of_month\n",
    "\n",
    "print('Declaración de funciones')\n",
    "def list_objects_function(buckets_, first_day, last_day, keys_, retrieve_last=False):\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(buckets_)\n",
    "    files_in_bucket = list(bucket.objects.all())\n",
    "    files_objets = [f\"s3://{buckets_}/\" + i.key for i in files_in_bucket if\n",
    "                        (i.key.find(keys_) >= 0) and (i.key.find('.parquet') >= 0)]\n",
    "    df_bucket_files = pd.DataFrame({\n",
    "            'key': [i[:(i.find('dt=') + 14)] for i in files_objets],\n",
    "            'path': files_objets,\n",
    "            'date': pd.to_datetime([i[(i.find('dt=') + 3):(i.find('dt=') + 13)] for i in files_objets])\n",
    "        })\n",
    "    files=list(df_bucket_files.loc[df_bucket_files['date'].between(str(first_day),str(last_day)),'path'].values)\n",
    "    return files\n",
    "\n",
    "def list_objects_cards(buckets_, keys_):\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(buckets_)\n",
    "    files_in_bucket = list(bucket.objects.all())\n",
    "    files_objets = [f\"s3://{buckets_}/\" + i.key for i in files_in_bucket if\n",
    "                        (i.key.find(keys_) >= 0) and (i.key.find('.parquet') >= 0)]\n",
    "    df_bucket_files = pd.DataFrame({\n",
    "            'key': [i for i in files_objets],\n",
    "            'path': files_objets\n",
    "        })\n",
    "    files=list(df_bucket_files.loc[:,'path'].values)\n",
    "    return files\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "first_day,last_day = first_and_last(today)\n",
    "print('Primer dia',first_day)\n",
    "print('Ultimo dia',last_day)\n",
    "\n",
    "#Transacciones obtenidas de bucket de survival\n",
    "files_objects_survival = list_objects_function(recommendations_bucket, first_day, last_day ,path_key_survival_stg)\n",
    "\n",
    "\n",
    "print(f'Hay {len(files_objects_survival)} archivos de survival en la carpeta')\n",
    "df_survival = spark.read.parquet(*files_objects_survival).select(['accountgp',  \n",
    "                            'vl_cashin_prestamos_sum', 'vl_cashin_adquirencia_sum',\n",
    "                            'nu_cashin_prestamos_qty',  'nu_cashin_adquirencia_qty',  \n",
    "                            'nu_tcc_r_aprob', 'nu_tcc_t_aprob', 'nu_tcc_z_aprob', 'vl_tcc_r_aprob', 'vl_tcc_t_aprob', 'vl_tcc_z_aprob', \n",
    "                            'nu_mode_digital_qty_0_aprob', 'nu_mode_digital_qty_1_aprob', 'nu_automatic_debit_aprob', 'nu_cash_out_cvu_aprob', \n",
    "                            'nu_consumption_pos_aprob', 'nu_investments_withdraw_aprob', 'nu_telerecargas_carga_aprob', 'nu_user_to_user_aprob', \n",
    "                            'nu_withdraw_atm_aprob', 'vl_automatic_debit_aprob', 'vl_cash_out_cvu_aprob', 'vl_consumption_pos_aprob', \n",
    "                            'vl_investments_withdraw_aprob', 'vl_telerecargas_carga_aprob', 'vl_user_to_user_aprob', 'vl_withdraw_atm_aprob', \n",
    "                            'nu_compras_aprob', 'nu_entretenimiento_aprob', 'nu_servicios_débitos_automaticos_aprob', 'nu_supermercados_alimentos_aprob',\n",
    "                            'nu_transferencias_retiros_aprob', 'vl_compras_aprob', 'vl_entretenimiento_aprob', 'vl_servicios_débitos_automaticos_aprob', \n",
    "                            'vl_supermercados_alimentos_aprob', 'vl_transferencias_retiros_aprob', 'nu_tcc_r_rech', 'nu_tcc_t_rech', 'nu_tcc_z_rech', \n",
    "                            'vl_tcc_r_rech', 'vl_tcc_t_rech', 'vl_tcc_z_rech', 'nu_mode_digital_qty_0_rech', 'nu_mode_digital_qty_1_rech', 'nu_automatic_debit_rech', \n",
    "                            'nu_cash_out_cvu_rech', 'nu_consumption_pos_rech', 'nu_investments_withdraw_rech', 'nu_telerecargas_carga_rech', 'nu_user_to_user_rech', \n",
    "                            'nu_withdraw_atm_rech', 'vl_automatic_debit_rech', 'vl_cash_out_cvu_rech', 'vl_consumption_pos_rech', 'vl_investments_withdraw_rech', \n",
    "                            'vl_telerecargas_carga_rech', 'vl_user_to_user_rech', 'vl_withdraw_atm_rech', 'nu_compras_rech', 'nu_entretenimiento_rech', \n",
    "                            'nu_servicios_débitos_automaticos_rech', 'nu_supermercados_alimentos_rech', 'nu_transferencias_retiros_rech', \n",
    "                            'vl_compras_rech', 'vl_entretenimiento_rech', 'vl_servicios_débitos_automaticos_rech', 'vl_supermercados_alimentos_rech', 'vl_transferencias_retiros_rech'])\n",
    "\n",
    "\n",
    "df_survival = df_survival.withColumnRenamed(\"nu_investments_withdraw_aprob\",\"nu_investments_deposit_aprob\")\\\n",
    "                            .withColumnRenamed(\"vl_investments_withdraw_aprob\", \"vl_investments_deposit_aprob\")\\\n",
    "                            .withColumnRenamed(\"nu_investments_withdraw_rech\",\"nu_investments_deposit_rech\")\\\n",
    "                            .withColumnRenamed(\"vl_investments_withdraw_rech\",\"vl_investments_deposit_rech\")\n",
    "\n",
    "\n",
    "#Datos Cards\n",
    "files_objects_cards = list_objects_cards(recommendations_bucket,path_key_cards)\n",
    "print(files_objects_cards)\n",
    "df_cards=spark.read.parquet(*files_objects_cards).select(['account_id','external_id']).dropDuplicates()\n",
    "\n",
    "\n",
    "#Datos amplitud\n",
    "files_objects_amplitude = list_objects_function(recommendations_bucket, first_day, last_day ,path_key_amplitude)\n",
    "df_amplitude = spark.read.parquet(*files_objects_amplitude)\n",
    "\n",
    "\n",
    "print('Size df_cards',df_cards.count())\n",
    "print('Fila Amplitude',df_amplitude.count())\n",
    "df_amplitude=df_amplitude.join(df_cards, df_amplitude[\"user_id\"] == df_cards[\"account_id\"], \"inner\")\n",
    "print('Filas Amplitude despues de inner join con cards',df_amplitude.count())\n",
    "df_amplitude=df_amplitude.join(df_survival, df_amplitude[\"external_id\"] == df_survival[\"accountgp\"], \"left\")\n",
    "print('Filas Amplitude despues de left join con transactions',df_amplitude.count())\n",
    "\n",
    "#LIMPIEZA INICIAL DE LOS DATOS\n",
    "columns_to_drop = ['accountgp', 'user_id']\n",
    "\n",
    "df_amplitude=df_amplitude.drop(*columns_to_drop).na.fill(0)\n",
    "\n",
    "\n",
    "#Guardamos info procesada en bucket de STAGE\n",
    "df_amplitude.write\\\n",
    "     .format('parquet')\\\n",
    "     .save(f's3://{recommendations_bucket}/data/stage/dt={str(first_day)}', mode='overwrite')\n",
    "\n",
    "#DELETE $FOLDER$\n",
    "\n",
    "def retrieve_files(path, file_type, list_dates):\n",
    "    bucket=path.split('/')[2]\n",
    "    prefix='/'.join(path.split('/')[3:])\n",
    "    list_objects=list(s3.Bucket(bucket).objects.all())\n",
    "    list_objects=[f's3://{bucket}/{i.key}' for i in list_objects if ((i.key.find(prefix)>=0) & any(x in i.key.lower() for x in list_dates) & (i.key.find(file_type)>=0))]\n",
    "    return list_objects\n",
    "\n",
    "\n",
    "delete_files = retrieve_files(path=f's3://{recommendations_bucket}/data/', file_type='$folder$', list_dates=['$folder$'])\n",
    "print('Files to delete', delete_files)\n",
    "files_keys=[]\n",
    "for i in range(0,len(delete_files)):\n",
    "    files_keys=files_keys+[{'Key':('/').join(delete_files[i].split('/')[3:])}]\n",
    "if len(files_keys)>0:\n",
    "    s3_client.delete_objects(Bucket=recommendations_bucket,\n",
    "                             Delete={'Objects':files_keys})\n",
    "del delete_files\n",
    "gc.collect()\n",
    "\n",
    "#print(df_cards.show(30))\n",
    "#print((df_cards.count(), len(df_cards.columns)))\n",
    "#print(df_cards.columns)\n",
    "#print(df_amplitude.show())\n",
    "#print(df_amplitude.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6783fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name='test-job_recommendations_stg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "34c0ecfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'JobName': 'test-job_recommendations_stg',\n",
       " 'ResponseMetadata': {'RequestId': 'd3d1a35f-c78b-4429-a3f0-04f8ad02c55a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Wed, 02 Jun 2021 20:15:36 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '42',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'd3d1a35f-c78b-4429-a3f0-04f8ad02c55a'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# borrar job\n",
    "glue.delete_job(\n",
    "    JobName=job_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3ae5d1",
   "metadata": {},
   "source": [
    "## 3. Generamos los parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6779f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = '2021-01-10'\n",
    "#bucket_survival='uala-arg-datalake-aiml-survival-dev'  ## AFIP, GP, etc\n",
    "recommendations_bucket='test-uala-arg-datalake-aiml-recommendations'  # Para outputs\n",
    "kms_key_arn='arn:aws:kms:us-east-1:322149183112:key/9cc44b23-c5e9-46cb-9987-0982d21f8d00' ## key para desencriptar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3557652a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".py uploaded\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Guardar el archivo .py\n",
    "s3.meta.client.upload_file('merge_stg.py', \n",
    "                           recommendations_bucket, #bucket\n",
    "                           'artifacts/code/stg/merge_stg.py' #key+filename\n",
    ")\n",
    "print('.py uploaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cd1b29",
   "metadata": {},
   "source": [
    "## 4. Creamos el job de GLUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "84f21dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = glue.create_job(Name=job_name, \n",
    "                      GlueVersion='2.0',\n",
    "                      Role='ML_AWSGlueServiceRole',\n",
    "                      Command={'Name': 'glueetl',\n",
    "                               'ScriptLocation': f's3://{recommendations_bucket}/artifacts/code/stg/merge_stg.py'},\n",
    "                      DefaultArguments={\n",
    "                        '--additional-python-modules': 'dateutil==2.8.1'},\n",
    "                      MaxCapacity=3\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "388a834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_run = glue.start_job_run(\n",
    "    JobName = job_name,\n",
    "    Arguments = {\n",
    "        '--today':today,\n",
    "        #'--bucket_survival': bucket_survival,\n",
    "        '--recommendations_bucket': recommendations_bucket,\n",
    "        '--kms_key_arn': kms_key_arn\n",
    "    } \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e667a2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JobRunId': 'jr_9c734b26eff52dbc7e3738a7364b9b5bb52e4b91788005546bee2f1564b8e5f5', 'ResponseMetadata': {'RequestId': '6b12bdaa-f783-43a4-8dd6-4fe34e93df1f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 02 Jun 2021 20:15:51 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '82', 'connection': 'keep-alive', 'x-amzn-requestid': '6b12bdaa-f783-43a4-8dd6-4fe34e93df1f'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(job_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02389745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "MAX_WAIT_TIME=time.time() + 60*10 # 1 hour\n",
    "max_time = time.time() + MAX_WAIT_TIME\n",
    "while time.time() < max_time:\n",
    "    response=glue.get_job_run(JobName=job_name, RunId=job_run['JobRunId'])\n",
    "    status = response['JobRun']['JobRunState']\n",
    "    print('Job run: {}'.format(status))\n",
    "    \n",
    "    if status == 'SUCCEEDED' or status == 'FAILED':\n",
    "        break\n",
    "        \n",
    "    time.sleep(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a242aa59",
   "metadata": {},
   "source": [
    "## 5. Corrida para todos los meses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "513ce7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: 2020-05-01\n",
      "A dormir!\n",
      "Procesando: 2020-06-01\n",
      "A dormir!\n",
      "Procesando: 2020-07-01\n",
      "A dormir!\n",
      "Procesando: 2020-08-01\n",
      "A dormir!\n",
      "Procesando: 2020-09-01\n",
      "A dormir!\n",
      "Procesando: 2020-10-01\n",
      "A dormir!\n",
      "Procesando: 2020-11-01\n",
      "A dormir!\n",
      "Procesando: 2020-12-01\n",
      "A dormir!\n",
      "Procesando: 2021-01-01\n",
      "A dormir!\n",
      "Procesando: 2021-02-01\n",
      "A dormir!\n",
      "Procesando: 2021-03-01\n",
      "A dormir!\n",
      "Procesando: 2021-04-01\n",
      "A dormir!\n"
     ]
    }
   ],
   "source": [
    "list_dates=['2020-05-01','2020-06-01','2020-07-01','2020-08-01','2020-09-01','2020-10-01'\n",
    "            ,'2020-11-01','2020-12-01','2021-01-01','2021-02-01','2021-03-01','2021-04-01']\n",
    "for row in list_dates:\n",
    "    #bucket_survival='uala-arg-datalake-aiml-survival-dev'  ## AFIP, GP, etc\n",
    "    print('Procesando:',row)\n",
    "    recommendations_bucket='test-uala-arg-datalake-aiml-recommendations'  # Para outputs\n",
    "    kms_key_arn='arn:aws:kms:us-east-1:322149183112:key/9cc44b23-c5e9-46cb-9987-0982d21f8d00' ## key para desencriptar\n",
    "    job_run = glue.start_job_run(\n",
    "        JobName = job_name,\n",
    "        Arguments = {\n",
    "            '--today':row,\n",
    "            #'--bucket_survival': bucket_survival,\n",
    "            '--recommendations_bucket': recommendations_bucket,\n",
    "            '--kms_key_arn': kms_key_arn\n",
    "        } \n",
    "    )\n",
    "    print('A dormir!')\n",
    "    time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218062f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
