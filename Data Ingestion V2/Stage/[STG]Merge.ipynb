{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c6b53f",
   "metadata": {},
   "source": [
    "# Proceso GLUE Merge Amplitude - 3022 - users - 7001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c423e7",
   "metadata": {},
   "source": [
    "## 1. Cargamos las librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9421c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55f895f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "import awswrangler as wr\n",
    "from itertools import chain\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "glue = boto3.client('glue')\n",
    "s3 = boto3.resource('s3')\n",
    "ssm = boto3.client('ssm') \n",
    "lakeformation = boto3.client('lakeformation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08262091",
   "metadata": {},
   "source": [
    "### 2. Armamos el proceso de glue para hacer el merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "955789fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting merge_stg.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile merge_stg.py\n",
    "\n",
    "import sys\n",
    "import pyspark.sql.functions as func\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "import boto3\n",
    "import ast\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import gc\n",
    "import sys\n",
    "from pyspark.conf import SparkConf\n",
    "import pandas as pd\n",
    "\n",
    "print('Lectura de parámetros')\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "print('NOW:', datetime.now())\n",
    "\n",
    "args = getResolvedOptions(sys.argv,\n",
    "                          ['today', \n",
    "                           'kms_key_arn', \n",
    "                           'recommendations_bucket'])\n",
    "\n",
    "recommendations_bucket = args['recommendations_bucket']\n",
    "kms_key_id = args['kms_key_arn']\n",
    "today = args['today']\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print('Spark Configuración')\n",
    "\n",
    "spark_conf = SparkConf().setAll([\n",
    "  (\"spark.hadoop.fs.s3.enableServerSideEncryption\", \"true\"),\n",
    "  (\"spark.hadoop.fs.s3.serverSideEncryption.kms.keyId\", kms_key_id)\n",
    "])\n",
    "\n",
    "sc = SparkContext(conf=spark_conf)\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "logger = glueContext.get_logger()\n",
    "\n",
    "\n",
    "\n",
    "print('Crear objetos S3-ssm')\n",
    "# ----------------------------------------------------------------------------------\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "ssm = boto3.client('ssm')\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "print('Parámetros:')\n",
    "path_key_survival_stg = 'data/raw/transactions/'\n",
    "path_key_amplitude = 'data/raw/amplitude/'\n",
    "path_key_cards = 'data/raw/cards/'\n",
    "path_key_users = 'data/raw/users/'\n",
    "path_key_accounts = 'data/raw/accounts/'\n",
    "path_key_1010 = 'data/raw/1010/'\n",
    "#s3://uala-arg-datalake-aiml-survival-dev/data/monthly_stage/\n",
    "## FECHAS INTERVALO\n",
    "#print('1. CALCULO DE FECHAS')\n",
    "##Today llevado al primero del mes menos 1 día\n",
    "#today = datetime.strptime(today, '%Y-%m-%d').date().replace(day=1)\n",
    "#last_day=(today-pd.offsets.DateOffset(days=1)).date()\n",
    "##\n",
    "#first_day=(last_day-pd.offsets.DateOffset(days=365)).date()\n",
    "#\n",
    "#print('2. Intevalo de fechas analizada: ',first_day,'y',last_day)\n",
    "\n",
    "def first_and_last(today):\n",
    "    fecha=datetime.strptime(today, '%Y-%m-%d').date()\n",
    "    first_day=fecha.replace(day=1)\n",
    "    next_month = fecha.replace(day=28) + timedelta(days=4)\n",
    "    last_day_of_month = next_month - timedelta(days=next_month.day)\n",
    "    return first_day,last_day_of_month\n",
    "\n",
    "print('Declaración de funciones')\n",
    "def list_objects_function(buckets_, first_day, last_day, keys_, retrieve_last=False):\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(buckets_)\n",
    "    files_in_bucket = list(bucket.objects.all())\n",
    "    files_objets = [f\"s3://{buckets_}/\" + i.key for i in files_in_bucket if\n",
    "                        (i.key.find(keys_) >= 0) and (i.key.find('.parquet') >= 0)]\n",
    "    df_bucket_files = pd.DataFrame({\n",
    "            'key': [i[:(i.find('dt=') + 14)] for i in files_objets],\n",
    "            'path': files_objets,\n",
    "            'date': pd.to_datetime([i[(i.find('dt=') + 3):(i.find('dt=') + 13)] for i in files_objets])\n",
    "        })\n",
    "    files=list(df_bucket_files.loc[df_bucket_files['date'].between(str(first_day),str(last_day)),'path'].values)\n",
    "    return files\n",
    "\n",
    "def list_objects(buckets_, keys_):\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(buckets_)\n",
    "    files_in_bucket = list(bucket.objects.all())\n",
    "    files_objets = [f\"s3://{buckets_}/\" + i.key for i in files_in_bucket if\n",
    "                        (i.key.find(keys_) >= 0) and (i.key.find('.parquet') >= 0)]\n",
    "    df_bucket_files = pd.DataFrame({\n",
    "            'key': [i for i in files_objets],\n",
    "            'path': files_objets\n",
    "        })\n",
    "    files=list(df_bucket_files.loc[:,'path'].values)\n",
    "    return files\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "first_day,last_day = first_and_last(today)\n",
    "print('Primer dia',first_day)\n",
    "print('Ultimo dia',last_day)\n",
    "\n",
    "#Transacciones obtenidas de bucket de survival\n",
    "files_objects_survival = list_objects_function(recommendations_bucket, first_day, last_day ,path_key_survival_stg)\n",
    "\n",
    "##### PARAM PROVINCE\n",
    "#dict_param_province = {\"gpcode\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25], \"gpname\":[\"Undefined\", \"capital federal\", \"gran buenos aires\", \"buenos aires\", \"catamarca\", \"cordoba\", \"corrientes\", \"chaco\", \"chubut\", \"entre rios\", \"formosa\", \"jujuy\", \"la pampa\", \"la rioja\", \"mendoza\", \"misiones\", \"neuquen\", \"rio negro\", \"salta\", \"san juan\", \"san luis\", \"santa fe\", \"santa cruz\", \"santiago del estero\", \"tierra del fuego\", \"tucuman\"]}\n",
    "#df_param_province = pd.DataFrame(dict_param_province)\n",
    "#print (df_param_province.head())\n",
    "#print (df_param_province.dtypes)\n",
    "\n",
    "dict_param_province = {0:\"Undefined\",\n",
    "1:\"capital federal\",\n",
    "2:\"gran buenos aires\",\n",
    "3:\"buenos aires\",\n",
    "4:\"catamarca\",\n",
    "5:\"cordoba\",\n",
    "6:\"corrientes\",\n",
    "7:\"chaco\",\n",
    "8:\"chubut\",\n",
    "9:\"entre rios\",\n",
    "10:\"formosa\",\n",
    "11:\"jujuy\",\n",
    "12:\"la pampa\",\n",
    "13:\"la rioja\",\n",
    "14:\"mendoza\",\n",
    "15:\"misiones\",\n",
    "16:\"neuquen\",\n",
    "17:\"rio negro\",\n",
    "18:\"salta\",\n",
    "19:\"san juan\",\n",
    "20:\"san luis\",\n",
    "21:\"santa fe\",\n",
    "22:\"santa cruz\",\n",
    "23:\"santiago del estero\",\n",
    "24:\"tierra del fuego\",\n",
    "25:\"tucuman\"}\n",
    "\n",
    "print(f'Hay {len(files_objects_survival)} archivos de survival en la carpeta')\n",
    "#df_survival = spark.read.parquet(*files_objects_survival).select(['accountgp',  \n",
    "#                            'vl_cashin_prestamos_sum', 'vl_cashin_adquirencia_sum',\n",
    "#                            'nu_cashin_prestamos_qty',  'nu_cashin_adquirencia_qty',  \n",
    "#                            'nu_tcc_r_aprob', 'nu_tcc_t_aprob', 'nu_tcc_z_aprob', 'vl_tcc_r_aprob', 'vl_tcc_t_aprob', 'vl_tcc_z_aprob', \n",
    "#                            'nu_mode_digital_qty_0_aprob', 'nu_mode_digital_qty_1_aprob', 'nu_automatic_debit_aprob', 'nu_cash_out_cvu_aprob', \n",
    "#                            'nu_consumption_pos_aprob', 'nu_investments_withdraw_aprob', 'nu_telerecargas_carga_aprob', 'nu_user_to_user_aprob', \n",
    "#                            'nu_withdraw_atm_aprob', 'vl_automatic_debit_aprob', 'vl_cash_out_cvu_aprob', 'vl_consumption_pos_aprob', \n",
    "#                            'vl_investments_withdraw_aprob', 'vl_telerecargas_carga_aprob', 'vl_user_to_user_aprob', 'vl_withdraw_atm_aprob', \n",
    "#                            'nu_compras_aprob', 'nu_entretenimiento_aprob', 'nu_servicios_débitos_automaticos_aprob', 'nu_supermercados_alimentos_aprob',\n",
    "#                            'nu_transferencias_retiros_aprob', 'vl_compras_aprob', 'vl_entretenimiento_aprob', 'vl_servicios_débitos_automaticos_aprob', \n",
    "#                            'vl_supermercados_alimentos_aprob', 'vl_transferencias_retiros_aprob', 'nu_tcc_r_rech', 'nu_tcc_t_rech', 'nu_tcc_z_rech', \n",
    "#                            'vl_tcc_r_rech', 'vl_tcc_t_rech', 'vl_tcc_z_rech', 'nu_mode_digital_qty_0_rech', 'nu_mode_digital_qty_1_rech', 'nu_automatic_debit_rech', \n",
    "#                            'nu_cash_out_cvu_rech', 'nu_consumption_pos_rech', 'nu_investments_withdraw_rech', 'nu_telerecargas_carga_rech', 'nu_user_to_user_rech', \n",
    "#                            'nu_withdraw_atm_rech', 'vl_automatic_debit_rech', 'vl_cash_out_cvu_rech', 'vl_consumption_pos_rech', 'vl_investments_withdraw_rech', \n",
    "#                            'vl_telerecargas_carga_rech', 'vl_user_to_user_rech', 'vl_withdraw_atm_rech', 'nu_compras_rech', 'nu_entretenimiento_rech', \n",
    "#                            'nu_servicios_débitos_automaticos_rech', 'nu_supermercados_alimentos_rech', 'nu_transferencias_retiros_rech', \n",
    "#                            'vl_compras_rech', 'vl_entretenimiento_rech', 'vl_servicios_débitos_automaticos_rech', 'vl_supermercados_alimentos_rech', 'vl_transferencias_retiros_rech'])\n",
    "\n",
    "df_survival = spark.read.parquet(*files_objects_survival)\n",
    "\n",
    "df_survival = df_survival.withColumnRenamed(\"nu_investments_withdraw_aprob\",\"nu_investments_deposit_aprob\")\\\n",
    "                            .withColumnRenamed(\"vl_investments_withdraw_aprob\", \"vl_investments_deposit_aprob\")\\\n",
    "                            .withColumnRenamed(\"nu_investments_withdraw_rech\",\"nu_investments_deposit_rech\")\\\n",
    "                            .withColumnRenamed(\"vl_investments_withdraw_rech\",\"vl_investments_deposit_rech\")\n",
    "\n",
    "\n",
    "#Datos Cards\n",
    "files_objects_cards = list_objects(recommendations_bucket,path_key_cards)\n",
    "#print(files_objects_cards)\n",
    "df_cards=spark.read.parquet(*files_objects_cards).select(['account_id','external_id']).dropDuplicates()\n",
    "#print(\"filas columnas cards\")\n",
    "#print((df_cards.count(), len(df_cards.columns)))\n",
    "\n",
    "#Datos 1010\n",
    "files_objects_1010 = list_objects(recommendations_bucket,path_key_1010)\n",
    "#print(files_objects_1010)\n",
    "df_1010=spark.read.parquet(*files_objects_1010).select(['numero_cuenta', 'fecha_alta', 'provincia', 'sexo', 'fecha_nacimiento']).dropDuplicates(['numero_cuenta'])\n",
    "#print(\"filas columnas 1010\")\n",
    "#print((df_1010.count(), len(df_1010.columns)))\n",
    "df_1010 = df_1010.withColumn(\"provincia\", df_1010[\"provincia\"].cast(IntegerType()))\n",
    "df_1010 = df_1010.withColumn(\"provincia\", df_1010[\"provincia\"].cast(StringType()))\n",
    "dict_param_province = {str(k):v for k,v in zip(dict_param_province.keys(), dict_param_province.values())}\n",
    "df_1010 = df_1010.na.replace(dict_param_province, 1, \"provincia\")\n",
    "#print(\"filas columnas 1010 post join\")\n",
    "#print((df_1010.count(), len(df_1010.columns)))\n",
    "#print(df_1010.show())\n",
    "#print(df_1010.dtypes)\n",
    "\n",
    "#Datos amplitude\n",
    "files_objects_amplitude = list_objects_function(recommendations_bucket, first_day, last_day ,path_key_amplitude)\n",
    "df_amplitude = spark.read.parquet(*files_objects_amplitude)\n",
    "\n",
    "#JOINS\n",
    "#print('Size df_cards',df_cards.count())\n",
    "#print('Fila Amplitude',df_amplitude.count())\n",
    "df_amplitude=df_amplitude.join(df_cards, df_amplitude[\"user_id\"] == df_cards[\"account_id\"], \"inner\")\n",
    "#print('Filas Amplitude despues de inner join con cards',df_amplitude.count())\n",
    "df_amplitude=df_amplitude.join(df_survival, df_amplitude[\"external_id\"] == df_survival[\"accountgp\"], \"left\")\n",
    "#print('Filas Amplitude despues de left join con transactions',df_amplitude.count())\n",
    "df_amplitude=df_amplitude.join(df_1010, df_amplitude[\"external_id\"] == df_1010[\"numero_cuenta\"], \"left\")\n",
    "#print('Filas Amplitude despues de left join con 1010',df_amplitude.count())\n",
    "\n",
    "#LIMPIEZA INICIAL DE LOS DATOS\n",
    "columns_to_drop = ['accountgp', 'numero_cuenta','user_id']\n",
    "df_amplitude=df_amplitude.drop(*columns_to_drop).na.fill(0)\n",
    "df_amplitude= df_amplitude.withColumn('fecha_nacimiento',F.to_date(df_amplitude.fecha_nacimiento,'yyyyMMdd'))\n",
    "df_amplitude= df_amplitude.withColumn('fecha_alta',F.to_date(df_amplitude.fecha_alta,'yyyyMMdd'))\n",
    "#print(df_amplitude.show())\n",
    "#print(df_amplitude.dtypes)\n",
    "#print((df_amplitude.count(), len(df_amplitude.columns)))\n",
    "#print('---------1010---------')\n",
    "#print(df_1010.show())\n",
    "#print(df_1010.dtypes)\n",
    "\n",
    "#Guardamos info procesada en bucket de STAGE\n",
    "df_amplitude.write\\\n",
    "     .format('parquet')\\\n",
    "     .save(f's3://{recommendations_bucket}/data/stage/splited_data_v2/dt={str(first_day)}', mode='overwrite')\n",
    "\n",
    "#DELETE $FOLDER$\n",
    "\n",
    "def retrieve_files(path, file_type, list_dates):\n",
    "    bucket=path.split('/')[2]\n",
    "    prefix='/'.join(path.split('/')[3:])\n",
    "    list_objects=list(s3.Bucket(bucket).objects.all())\n",
    "    list_objects=[f's3://{bucket}/{i.key}' for i in list_objects if ((i.key.find(prefix)>=0) & any(x in i.key.lower() for x in list_dates) & (i.key.find(file_type)>=0))]\n",
    "    return list_objects\n",
    "\n",
    "\n",
    "delete_files = retrieve_files(path=f's3://{recommendations_bucket}/data/', file_type='$folder$', list_dates=['$folder$'])\n",
    "print('Files to delete', delete_files)\n",
    "files_keys=[]\n",
    "for i in range(0,len(delete_files)):\n",
    "    files_keys=files_keys+[{'Key':('/').join(delete_files[i].split('/')[3:])}]\n",
    "if len(files_keys)>0:\n",
    "    s3_client.delete_objects(Bucket=recommendations_bucket,\n",
    "                             Delete={'Objects':files_keys})\n",
    "del delete_files\n",
    "gc.collect()\n",
    "\n",
    "#print(df_cards.show(30))\n",
    "#print((df_cards.count(), len(df_cards.columns)))\n",
    "#print(df_cards.columns)\n",
    "#print(df_amplitude.show())\n",
    "#print(df_amplitude.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c6c32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name='test-job_recommendations_stg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdd09c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'JobName': 'test-job_recommendations_stg',\n",
       " 'ResponseMetadata': {'RequestId': '9a6d547d-314b-4e82-8f48-bab0e680497b',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Wed, 23 Jun 2021 14:01:14 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '42',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '9a6d547d-314b-4e82-8f48-bab0e680497b'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# borrar job\n",
    "glue.delete_job(\n",
    "    JobName=job_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28641f",
   "metadata": {},
   "source": [
    "## 3. Generamos los parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c77f1585",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = '2021-01-10'\n",
    "#bucket_survival='uala-arg-datalake-aiml-survival-dev'  ## AFIP, GP, etc\n",
    "recommendations_bucket='test-uala-arg-datalake-aiml-recommendations'  # Para outputs\n",
    "kms_key_arn='arn:aws:kms:us-east-1:322149183112:key/9cc44b23-c5e9-46cb-9987-0982d21f8d00' ## key para desencriptar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eafef3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".py uploaded\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Guardar el archivo .py\n",
    "s3.meta.client.upload_file('merge_stg.py', \n",
    "                           recommendations_bucket, #bucket\n",
    "                           'artifacts/code/stg/merge_stg.py' #key+filename\n",
    ")\n",
    "print('.py uploaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420058b3",
   "metadata": {},
   "source": [
    "## 4. Creamos el job de GLUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84b62e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = glue.create_job(Name=job_name, \n",
    "                      GlueVersion='2.0',\n",
    "                      Role='ML_AWSGlueServiceRole',\n",
    "                      Command={'Name': 'glueetl',\n",
    "                               'ScriptLocation': f's3://{recommendations_bucket}/artifacts/code/stg/merge_stg.py'},\n",
    "                      DefaultArguments={\n",
    "                        '--additional-python-modules': 'dateutil==2.8.1'},\n",
    "                      MaxCapacity=3\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "954dda96",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_run = glue.start_job_run(\n",
    "    JobName = job_name,\n",
    "    Arguments = {\n",
    "        '--today':today,\n",
    "        #'--bucket_survival': bucket_survival,\n",
    "        '--recommendations_bucket': recommendations_bucket,\n",
    "        '--kms_key_arn': kms_key_arn\n",
    "    } \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0408950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JobRunId': 'jr_b381a4aa4f569df04d962a41a5c77f77ba56b459efd659ce528a64cd388a9904', 'ResponseMetadata': {'RequestId': '493d0cc3-4e68-4e08-a0c1-1899d377db68', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 23 Jun 2021 14:02:33 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '82', 'connection': 'keep-alive', 'x-amzn-requestid': '493d0cc3-4e68-4e08-a0c1-1899d377db68'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(job_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1c5874c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: RUNNING\n",
      "Job run: SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "MAX_WAIT_TIME=time.time() + 60*10 # 1 hour\n",
    "max_time = time.time() + MAX_WAIT_TIME\n",
    "while time.time() < max_time:\n",
    "    response=glue.get_job_run(JobName=job_name, RunId=job_run['JobRunId'])\n",
    "    status = response['JobRun']['JobRunState']\n",
    "    print('Job run: {}'.format(status))\n",
    "    \n",
    "    if status == 'SUCCEEDED' or status == 'FAILED':\n",
    "        break\n",
    "        \n",
    "    time.sleep(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede676b3",
   "metadata": {},
   "source": [
    "## 5. Corrida para todos los meses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "752cbe93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: 2021-04-01\n",
      "A dormir!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8e3bfb393696>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'A dormir!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#list_dates=['2020-05-01','2020-06-01','2020-07-01','2020-08-01','2020-09-01','2020-10-01'\n",
    "#            ,'2020-11-01','2020-12-01','2021-01-01','2021-02-01','2021-03-01','2021-04-01']\n",
    "list_dates=['2021-04-01']\n",
    "for row in list_dates:\n",
    "    #bucket_survival='uala-arg-datalake-aiml-survival-dev'  ## AFIP, GP, etc\n",
    "    print('Procesando:',row)\n",
    "    recommendations_bucket='test-uala-arg-datalake-aiml-recommendations'  # Para outputs\n",
    "    kms_key_arn='arn:aws:kms:us-east-1:322149183112:key/9cc44b23-c5e9-46cb-9987-0982d21f8d00' ## key para desencriptar\n",
    "    job_run = glue.start_job_run(\n",
    "        JobName = job_name,\n",
    "        Arguments = {\n",
    "            '--today':row,\n",
    "            #'--bucket_survival': bucket_survival,\n",
    "            '--recommendations_bucket': recommendations_bucket,\n",
    "            '--kms_key_arn': kms_key_arn\n",
    "        } \n",
    "    )\n",
    "    print('A dormir!')\n",
    "    time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f77ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f91fdf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4380ab97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
