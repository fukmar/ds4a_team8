{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22175fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a354d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0f7203",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lectura users')\n",
    "\n",
    "path_users = list_objects_function(buckets_ = datalake_prod_bucket, \n",
    "                                   keys_ = gp1010_key, \n",
    "                                   retrieve_date = '', \n",
    "                                   date_logic = 'all')\n",
    "\n",
    "columnas_usr = ['numero_cuenta', 'estado_cuenta', 'fecha_alta', 'fecha_nacimiento', 'sexo', 'provincia']\n",
    "df_usr = spark.read.parquet(*path_users).select(columnas_usr)\n",
    "\n",
    "#print(f'Hay {df_usr.count()} users')\n",
    "df_usr = df_usr.dropDuplicates(['numero_cuenta'])\n",
    "#print(f'Hay {df_usr.count()} users luego de sacar duplicados')\n",
    "\n",
    "# dtypes\n",
    "df_usr = (df_usr\n",
    "          .withColumn('numero_cuenta', df_usr.numero_cuenta.cast('string'))\n",
    "          .withColumn('fecha_alta', F.to_date(df_usr.fecha_alta,\"yyyy-MM-dd\"))\n",
    "          .withColumn('fecha_nacimiento', F.to_date(df_usr.fecha_nacimiento,\"yyyy-MM-dd\"))\n",
    "          .withColumn('provincia', df_usr.provincia.cast('int'))\n",
    "         )\n",
    "\n",
    "# Edad y antiguedad\n",
    "df_usr = (df_usr\n",
    "          .withColumn('edad', F.floor(F.datediff(F.current_date(), F.col('fecha_nacimiento'))/365.25))\n",
    "          .withColumn('antiguedad', F.datediff(F.current_date(), F.col('fecha_alta')))\n",
    "         )\n",
    "          \n",
    "\n",
    "          \n",
    "df_usr = df_usr.withColumnRenamed('numero_cuenta','externalid')\n",
    "\n",
    "# Provincia int a str\n",
    "prov_dict = {\n",
    "    1: 'CAPITAL_FEDERAL',\n",
    "    2: 'BUENOS_AIRES',  # GRAN BUENOS AIRES\n",
    "    3: 'BUENOS_AIRES',\n",
    "    4: 'OTROS',  # CATAMARCA\n",
    "    5: 'CORDOBA',\n",
    "    6: 'NORTE_ESTE',  # CORRIENTES\n",
    "    7: 'NORTE_ESTE',  # CHACO\n",
    "    8: 'OTROS',  # CHUBUT\n",
    "    9: 'OTROS',  # ENTRE RIOS\n",
    "    10: 'NORTE_ESTE',  # FORMOSA\n",
    "    11: 'NORTE',  # JUJUY\n",
    "    12: 'OTROS',  # LA PAMPA\n",
    "    13: 'OTROS',  # LA RIOJA\n",
    "    14: 'MENDOZA',\n",
    "    15: 'NORTE_ESTE',  # MISIONES\n",
    "    16: 'OTROS',  # NEUQUEN\n",
    "    17: 'OTROS',  # RIO NEGRO\n",
    "    18: 'NORTE',  # SALTA\n",
    "    19: 'OTROS',  # SAN JUAN\n",
    "    20: 'OTROS',  # SAN LUIS\n",
    "    21: 'SANTA_FE',\n",
    "    22: 'OTROS',  # SANTA CRUZ\n",
    "    23: 'OTROS',  # SANTIAGO DEL ESTERO\n",
    "    24: 'OTROS',  # TIERRA DEL FUEGO\n",
    "    25: 'NORTE'  # TUCUMAN\n",
    "}\n",
    "\n",
    "prov_mapping = F.create_map([F.lit(x) for x in chain(*prov_dict.items())])\n",
    "df_usr = df_usr.withColumn('provincia', prov_mapping[df_usr['provincia']])\n",
    "\n",
    "print('Uni√≥n con mappeo id')\n",
    "\n",
    "df_usr = df_usr.join(df_mapeo_id_gp, df_usr.externalid == df_mapeo_id_gp.ext_id_mappeo, how='left')\n",
    "df_usr = df_usr.drop('ext_id_mappeo')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
