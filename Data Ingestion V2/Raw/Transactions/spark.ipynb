{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b6dcce5",
   "metadata": {},
   "source": [
    "# Job Glue ETL Adquirencia - Fraude\n",
    "\n",
    "Fuentes: \n",
    "- Datalake prod: uala-arg-datalake-analytics-prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eea2331c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e9172c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a01c149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "glue = boto3.client('glue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cb7a02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting job_adquirencia.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile job_adquirencia.py\n",
    "\n",
    "#####################\n",
    "####### Libs ########\n",
    "#####################\n",
    "#!pip install -upgrade boto3\n",
    "print('Importar librerías')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "#import math\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "from itertools import chain\n",
    "import awswrangler as wr\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta, SU\n",
    "\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from awsglue.job import Job\n",
    "from awsglue.context import GlueContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import boto3\n",
    "ssm = boto3.client('ssm') \n",
    "###\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=com.amazonaws:aws-java-sdk-bundle:1.11.271,org.apache.hadoop:hadoop-aws:3.1.2 pyspark-shell\"\n",
    "###\n",
    "\n",
    "\n",
    "#####################\n",
    "####### Params ######\n",
    "#####################\n",
    "print('Leer parámetros')\n",
    "args = getResolvedOptions(sys.argv,\n",
    "                          ['adquirencia_bucket',\n",
    "                           'adquirencia_key_inf', \n",
    "                           'datalake_prod_bucket', \n",
    "                           'transactions_key',    # bucket prod datalake\n",
    "                           'gp1010_key',          # bucket prod datalake\n",
    "                           'cotizaciones_bucket', # momentáneamente en auxiliar\n",
    "                           'cotizaciones_key',\n",
    "                           'accountid_gp_bucket', # momentáneamente en auxiliar\n",
    "                           'accountid_gp_key'\n",
    "                           #'last_run_ps_name',\n",
    "                           #'kms_key_arn'\n",
    "                           ])\n",
    "\n",
    "adquirencia_bucket   = args['adquirencia_bucket']\n",
    "adquirencia_key_inf  = args['adquirencia_key_inf']\n",
    "datalake_prod_bucket = args['datalake_prod_bucket']\n",
    "transactions_key     = args['transactions_key']\n",
    "gp1010_key           = args['gp1010_key']\n",
    "cotizaciones_bucket  = args['cotizaciones_bucket']\n",
    "cotizaciones_key     = args['cotizaciones_key']\n",
    "accountid_gp_bucket  = args['accountid_gp_bucket']\n",
    "accountid_gp_key     = args['accountid_gp_key']\n",
    "\n",
    "#kms_key_arn         = args['kms_key_arn']\n",
    "#last_run_ps_name = args['last_run_ps_name']\n",
    "\n",
    "now = datetime.now().date()\n",
    "print(f'Now = {now}')\n",
    "now = pd.to_datetime(now)\n",
    "\n",
    "prior_6_month = (now - pd.DateOffset(months=6)).strftime('%Y-%m-%d')\n",
    "print(f'prior 6m = {prior_6_month}')\n",
    "\n",
    "one_week_prior = now - pd.Timedelta(7, 'd')\n",
    "#one_week_prior = one_week_prior.strftime('%Y-%m-%d')\n",
    "print(f'prior 1 week: {one_week_prior}')\n",
    "\n",
    "\n",
    "######################\n",
    "##### Conexiones #####\n",
    "######################\n",
    "\n",
    "######### Boto  #############\n",
    "print('Request keys boto3 client & resource')\n",
    "#glue = boto3.client('glue')\n",
    "sts = boto3.client('sts')\n",
    "\n",
    "# Conexiones para listar archivos \n",
    "response = sts.assume_role(\n",
    "    RoleArn         = 'arn:aws:iam::514405401387:role/aws-rol-ml-read-stage-prod', #es el rol que existe en produccion por el cual \"nos hacemos pasar\" para acceder a los buckets de s3\n",
    "    RoleSessionName = 'sesion-adq', # nombre que le damos a la sesión\n",
    "    DurationSeconds = 3600 # es el tiempo que dura la sesion por default si no especificamos este parámetro.\n",
    ")\n",
    "\n",
    "s3c = boto3.client('s3',\n",
    "    aws_access_key_id     = response['Credentials']['AccessKeyId'],\n",
    "    aws_secret_access_key = response['Credentials']['SecretAccessKey'],\n",
    "    aws_session_token     = response['Credentials']['SessionToken']\n",
    ")\n",
    "\n",
    "s3r = boto3.resource('s3',\n",
    "    aws_access_key_id     = response['Credentials']['AccessKeyId'],\n",
    "    aws_secret_access_key = response['Credentials']['SecretAccessKey'],\n",
    "    aws_session_token     = response['Credentials']['SessionToken']\n",
    ")\n",
    "\n",
    "\n",
    "######### Spark  #############\n",
    "\n",
    "print('Request keys Spark y configuracion')\n",
    "response_spark = sts.assume_role(RoleArn = 'arn:aws:iam::514405401387:role/aws-rol-ml-read-stage-prod', #es el rol que existe en produccion por el cual \"nos hacemos pasar\" para acceder a los buckets de s3\n",
    "                                 RoleSessionName = 'sesion-adq-spark', # nombre que le damos a la sesión\n",
    "                                 DurationSeconds = 3600 # es el tiempo que dura la sesion por default si no especificamos este parámetro.\n",
    "                                )\n",
    "\n",
    "spark_conf = SparkConf().setAll([\n",
    "  (\"spark.hadoop.fs.s3.enableServerSideEncryption\", \"true\")\n",
    "  #(\"spark.hadoop.fs.s3.serverSideEncryption.kms.keyId\", kms_key_id)\n",
    "                                ])\n",
    "\n",
    "sc = SparkContext(conf=spark_conf)\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "logger = glueContext.get_logger()\n",
    "\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\", \n",
    "                                     \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\",     response_spark[\"Credentials\"][\"AccessKeyId\"])\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",     response_spark[\"Credentials\"][\"SecretAccessKey\"])\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.session.token\",  response_spark[\"Credentials\"][\"SessionToken\"])\n",
    "\n",
    "\n",
    "#####################\n",
    "##### Functions #####\n",
    "#####################\n",
    "\n",
    "def list_objects_function(buckets_, keys_, retrieve_date='', date_logic='equal'):\n",
    "    \"\"\"\n",
    "    date_ (str) : Fecha para buscar parquets.\n",
    "    date_logic (str) :  Si 'last', devuelve una lista con los path de los parquets de la carpeta más reciente y la fecha correspondiente. \n",
    "                        Si 'great_equal', devuelve una lista con los path de los parquets de las carpetas >= date_.\n",
    "                        Si 'equal',  devuelve una lista con los path de los parquets de las carpetas == date_.\n",
    "                        Si 'all', devuelve una lista con todos los paths de los parquets \n",
    "    \"\"\"\n",
    "   \n",
    "\n",
    "    paginator = s3c.get_paginator('list_objects_v2')\n",
    "    pages     = paginator.paginate(Bucket=buckets_, \n",
    "                                   Prefix=keys_)\n",
    "    files_in_bucket=[]\n",
    "    for page in pages:\n",
    "        files_page = [key['Key'] for key in page['Contents']]\n",
    "        files_in_bucket+=files_page\n",
    "    #s3a al leer cross account, sino  s3\n",
    "    files_objets = [f\"s3a://{buckets_}/\" + i for i in files_in_bucket if i.find('.parquet') >= 0]\n",
    "    \n",
    "    df_bucket_files = pd.DataFrame({\n",
    "        'key': [i[:(i.find('dt=') + 14)] for i in files_objets],\n",
    "        'path': files_objets,\n",
    "        'date': pd.to_datetime([i[(i.find('dt=') + 3):(i.find('dt=') + 13)] for i in files_objets])\n",
    "    })\n",
    "\n",
    "    if date_logic == 'last':\n",
    "        max_date = df_bucket_files['date'].max().strftime('%Y-%m-%d')\n",
    "        files = df_bucket_files.query('date == @max_date')['path'].tolist()\n",
    "        return files, max_date\n",
    "    # Para transactions:\n",
    "    elif date_logic == 'great_equal':\n",
    "        files = df_bucket_files.query(\n",
    "            'date >= @retrieve_date')['path'].to_list()\n",
    "        return files\n",
    "    \n",
    "    elif date_logic == 'equal':\n",
    "        files = df_bucket_files.query(\n",
    "            'date == @retrieve_date')['path'].to_list()\n",
    "        return files\n",
    "    \n",
    "    elif date_logic == 'all':\n",
    "        return df_bucket_files['path'].to_list()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "################################\n",
    "##  accountID <--> accountGP  ##\n",
    "################################\n",
    "print('Lectura mapeo accountID - accountGP')\n",
    "\n",
    "mapeo_id_gp = wr.s3.read_parquet(f's3://{accountid_gp_bucket}/{accountid_gp_key}/account_id_gp.parquet')\n",
    "df_mapeo_id_gp = spark.createDataFrame(mapeo_id_gp)\n",
    "df_mapeo_id_gp = df_mapeo_id_gp.withColumn('externalid', df_mapeo_id_gp.externalid.cast('string'))\n",
    "\n",
    "\n",
    "################################\n",
    "#######  Lectura users   #######\n",
    "################################\n",
    "\n",
    "print('Lectura users')\n",
    "\n",
    "path_users = list_objects_function(buckets_ = datalake_prod_bucket, \n",
    "                                   keys_ = gp1010_key, \n",
    "                                   retrieve_date = '', \n",
    "                                   date_logic = 'all')\n",
    "\n",
    "columnas_usr = ['numero_cuenta', 'estado_cuenta', 'fecha_alta', 'fecha_nacimiento', 'sexo', 'provincia']\n",
    "df_usr = spark.read.parquet(*path_users).select(columnas_usr)\n",
    "\n",
    "print(f'Hay {df_usr.count()} users')\n",
    "df_usr = df_usr.dropDuplicates(['numero_cuenta'])\n",
    "print(f'Hay {df_usr.count()} users luego de sacar duplicados')\n",
    "\n",
    "# dtypes\n",
    "df_usr = (df_usr\n",
    "          .withColumn('numero_cuenta', df_usr.numero_cuenta.cast('string'))\n",
    "          .withColumn('fecha_alta', F.to_date(df_usr.fecha_alta,\"yyyy-MM-dd\"))\n",
    "          .withColumn('fecha_nacimiento', F.to_date(df_usr.fecha_nacimiento,\"yyyy-MM-dd\"))\n",
    "          .withColumn('provincia', df_usr.provincia.cast('int'))\n",
    "         )\n",
    "\n",
    "# Edad y antiguedad\n",
    "df_usr = (df_usr\n",
    "          .withColumn('edad', F.floor(F.datediff(F.current_date(), F.col('fecha_nacimiento'))/365.25))\n",
    "          .withColumn('antiguedad', F.datediff(F.current_date(), F.col('fecha_alta')))\n",
    "         )\n",
    "\n",
    "# Provincia int a str\n",
    "prov_dict = {\n",
    "    1: 'CAPITAL_FEDERAL',\n",
    "    2: 'BUENOS_AIRES',  # GRAN BUENOS AIRES\n",
    "    3: 'BUENOS_AIRES',\n",
    "    4: 'OTROS',  # CATAMARCA\n",
    "    5: 'CORDOBA',\n",
    "    6: 'NORTE_ESTE',  # CORRIENTES\n",
    "    7: 'NORTE_ESTE',  # CHACO\n",
    "    8: 'OTROS',  # CHUBUT\n",
    "    9: 'OTROS',  # ENTRE RIOS\n",
    "    10: 'NORTE_ESTE',  # FORMOSA\n",
    "    11: 'NORTE',  # JUJUY\n",
    "    12: 'OTROS',  # LA PAMPA\n",
    "    13: 'OTROS',  # LA RIOJA\n",
    "    14: 'MENDOZA',\n",
    "    15: 'NORTE_ESTE',  # MISIONES\n",
    "    16: 'OTROS',  # NEUQUEN\n",
    "    17: 'OTROS',  # RIO NEGRO\n",
    "    18: 'NORTE',  # SALTA\n",
    "    19: 'OTROS',  # SAN JUAN\n",
    "    20: 'OTROS',  # SAN LUIS\n",
    "    21: 'SANTA_FE',\n",
    "    22: 'OTROS',  # SANTA CRUZ\n",
    "    23: 'OTROS',  # SANTIAGO DEL ESTERO\n",
    "    24: 'OTROS',  # TIERRA DEL FUEGO\n",
    "    25: 'NORTE'  # TUCUMAN\n",
    "}\n",
    "\n",
    "prov_mapping = F.create_map([F.lit(x) for x in chain(*prov_dict.items())])\n",
    "df_usr = df_usr.withColumn('provincia', prov_mapping[df_usr['provincia']])\n",
    "\n",
    "\n",
    "################################\n",
    "##### Lectura cotizaciones #####\n",
    "################################\n",
    "\n",
    "print('Lectura cotizaciones')\n",
    "\n",
    "cotizaciones = wr.s3.read_parquet(f's3://{cotizaciones_bucket}/{cotizaciones_key}/df_cotizacion.parquet')\n",
    "\n",
    "print(f'Fecha más nueva con cotizaciones: {cotizaciones.fecha.max()}')\n",
    "df_cotizaciones = spark.createDataFrame(cotizaciones)\n",
    "df_cotizaciones = df_cotizaciones.withColumn('fecha', F.to_date(df_cotizaciones.fecha))\n",
    "\n",
    "\n",
    "################################\n",
    "##### Lectura transactions #####\n",
    "################################\n",
    "\n",
    "print('Lectura transactions')\n",
    "path_transactions = list_objects_function(buckets_ = datalake_prod_bucket, \n",
    "                                          keys_ = transactions_key, \n",
    "                                          retrieve_date = prior_6_month, \n",
    "                                          date_logic = 'great_equal')\n",
    "print(f'{len(path_transactions)} parquets de trx encontrados')\n",
    "\n",
    "columnas_transactions = [\"transaction_date\",\"account_from\", \"account_to\", \"amount\", \n",
    "                         \"transaction_type\", \"status\", \"transaction_id\"]\n",
    "\n",
    "df_trx = spark.read.parquet(*path_transactions).select(columnas_transactions)\n",
    "\n",
    "df_trx = df_trx.dropDuplicates(['transaction_id'])\n",
    "print(f'Hay {df_trx.count()} transacciones luego de sacar duplicados')\n",
    "\n",
    "\n",
    "### Transformaciones ###\n",
    "########################\n",
    "\n",
    "#df = df.withColumn('transaction_date', F.to_timestamp(df.transaction_date, \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"))\n",
    "df_trx = df_trx.withColumn('transaction_date', F.to_date(df_trx.transaction_date))\n",
    "# Filtramos por status. Ver qué hacer con \"CANCELED\"\n",
    "df_trx = df_trx.filter(F.col('status').isin(['AUTHORIZED', 'REJECTED']))\n",
    "\n",
    "### Generar columna mes agrupacion\n",
    "def gen_interval(date):\n",
    "    cotas = [(now - pd.DateOffset(months=i)) for i in range(1,7)]\n",
    "    if (date >= cotas[0]):\n",
    "        return '6'\n",
    "    elif date >= cotas[1]:\n",
    "        return '5'\n",
    "    elif date >= cotas[2]:\n",
    "        return '4'\n",
    "    elif date >= cotas[3]:\n",
    "        return '3'\n",
    "    elif date >= cotas[4]:\n",
    "        return '2'\n",
    "    elif date >= cotas[5]:\n",
    "        return '1'\n",
    "\n",
    "col_date_interval = F.udf(gen_interval, StringType())\n",
    "df_trx = df_trx.withColumn('transaction_month', col_date_interval('transaction_date'))\n",
    "print(df_trx.select(['transaction_date', 'transaction_month']).show(2))\n",
    "\n",
    "### Pasar a USD\n",
    "df_trx = df_trx.join(df_cotizaciones,  df_trx.transaction_date == df_cotizaciones.fecha, how='left')\n",
    "df_trx = (df_trx\n",
    "          .drop(F.col('fecha'))\n",
    "          .withColumn('amount_usd', F.col('amount')/F.col('venta_uala'))\n",
    "         )\n",
    "\n",
    "\n",
    "# USER_TO_USER_IN\n",
    "df_trx_user_user_in = df_trx.filter(df_trx.transaction_type == \"USER_TO_USER\")\n",
    "df_trx_user_user_in = df_trx_user_user_in.drop(F.col('account_from'))\n",
    "df_trx_user_user_in = df_trx_user_user_in.withColumnRenamed('account_to','account_id')\n",
    "df_trx_user_user_in = df_trx_user_user_in.withColumn('transaction_type', F.lit('USER_TO_USER_IN'))\n",
    "df_trx = df_trx.drop(F.col('account_to'))\n",
    "df_trx = df_trx.withColumnRenamed('account_from','account_id')\n",
    "df_trx = df_trx.union(df_trx_user_user_in)\n",
    "\n",
    "\n",
    "### 6 meses ###\n",
    "\n",
    "print('Creación variables 6 meses')\n",
    "df_trx_6m = (df_trx\n",
    "             # a string porque sino groupby lo agrega\n",
    "             .withColumn('cashin_tdc_6m', F.when((F.col('transaction_type') == 'CASH_IN_TDC') & (F.col('status') == 'AUTHORIZED'), F.col('amount_usd')).otherwise(0))\n",
    "             .withColumn('cashin_to_user_6m', F.when((F.col('transaction_type') == 'USER_TO_USER_IN') & (F.col('status') == 'AUTHORIZED'), F.col('amount_usd')).otherwise(0))\n",
    "             .withColumn('cashin_cvu_6m', F.when((F.col('transaction_type') == 'CASH_IN_CVU') & (F.col('status') == 'AUTHORIZED'), F.col('amount_usd')).otherwise(0))\n",
    "             .withColumn('cashin_investments_withdraw_6m', F.when((F.col('transaction_type') == 'INVESTMENTS_WITHDRAW') & (F.col('status') == 'AUTHORIZED'), F.col('amount_usd')).otherwise(0))\n",
    "             .withColumn('cashout_purchase_6m', F.when((F.col('transaction_type') == 'CONSUMPTION_POS') & (F.col('status') == 'AUTHORIZED'), F.col('amount_usd')).otherwise(0))\n",
    "             .withColumn('cashout_cvu_6m', F.when((F.col('transaction_type') == 'CASH_OUT_CVU') & (F.col('status') == 'AUTHORIZED'), F.col('amount_usd')).otherwise(0))\n",
    "             .withColumn('cashout_user_to_user_6m', F.when((F.col('transaction_type') == 'USER_TO_USER') & (F.col('status') == 'AUTHORIZED'), F.col('amount_usd')).otherwise(0))\n",
    "             .withColumn('cashout_telerecarga_6m', F.when((F.col('transaction_type') == 'TELERECARGAS_CARGA') & (F.col('status') == 'AUTHORIZED'), F.col('amount_usd')).otherwise(0))\n",
    "             .withColumn('cashout_automatic_debit_6m', F.when((F.col('transaction_type') == 'AUTOMATIC_DEBIT') & (F.col('status') == 'AUTHORIZED'), F.col('amount_usd')).otherwise(0))\n",
    "             .withColumn('cashout_withdraw_atm_6m', F.when((F.col('transaction_type') == 'WITHDRAW_ATM') & (F.col('status') == 'AUTHORIZED'), F.col('amount_usd')).otherwise(0))\n",
    "             .withColumn('cashout_rechazadas_con_ad_6m', F.when((F.col('transaction_type') == 'AUTOMATIC_DEBIT') & (F.col('status') == 'REJECTED'), F.col('amount_usd')).otherwise(0))\n",
    "             .withColumn('cashout_rechazadas_sin_ad_6m', F.when((F.col('transaction_type').isin(['CONSUMPTION_POS', 'CASH_OUT_CVU', 'USER_TO_USER', 'TELERECARGAS_CARGA', 'WITHDRAW_ATM'])) &\n",
    "                                                                (F.col('status') == 'REJECTED'), F.col('amount_usd')).otherwise(0))\n",
    "             )\n",
    "\n",
    "print('Groupby por accountID y mes')\n",
    "cols_cash = ['account_id', 'transaction_month'] + [s for s in df_trx_6m.columns if 'cash' in s]\n",
    "df_trx_6m = (df_trx_6m.select(cols_cash).groupBy(['account_id', 'transaction_month']).sum()).toDF(*cols_cash)\n",
    "print('primer groupby 6m exitoso')\n",
    "#print(f'{df_trx_6m.count()} users con trx 6m')\n",
    "\n",
    "df_pandas = df_trx_6m.toPandas()\n",
    "wr.s3.to_parquet(\n",
    "                 df=df_pandas,\n",
    "                 path=(f's3://{adquirencia_bucket}/data/test/df_6m.parquet')\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "# ##\n",
    "# print('Copiamos solo 1 mes de trx')\n",
    "# cols_cash = ['account_id', 'transaction_month'] + [s.replace('6','1') for s in df_trx_6m.columns if 'cash' in s]\n",
    "# df_trx_1m = df_trx_6m.filter(df_trx_6m.transaction_month == '6').toDF(*cols_cash)\n",
    "# #print(f'{df_trx_1m.count()} users con trx 1m')\n",
    "\n",
    "# ##\n",
    "\n",
    "# print('Groupby por accountID')\n",
    "# cols_cash = ['account_id'] + [s for s in df_trx_6m.columns if 'cash' in s]\n",
    "\n",
    "# # df_trx_6m = df_trx_6m.select(cols_cash).groupBy('account_id').agg(\n",
    "# #     F.expr('percentile(cashin_tdc_6m, array(0.5))')[0].alias('cashin_tdc_6m'),\n",
    "# #     F.expr('percentile(cashin_to_user_6m, array(0.5))')[0].alias('cashin_to_user_6m'),\n",
    "# #     F.expr('percentile(cashin_cvu_6m, array(0.5))')[0].alias('cashin_cvu_6m'),\n",
    "# #     F.expr('percentile(cashin_investments_withdraw_6m, array(0.5))')[0].alias('cashin_investments_withdraw_6m'),\n",
    "# #     F.expr('percentile(cashout_purchase_6m, array(0.5))')[0].alias('cashout_purchase_6m'),\n",
    "# #     F.expr('percentile(cashout_cvu_6m, array(0.5))')[0].alias('cashout_cvu_6m'),\n",
    "# #     F.expr('percentile(cashout_user_to_user_6m, array(0.5))')[0].alias('cashout_user_to_user_6m'),\n",
    "# #     F.expr('percentile(cashout_telerecarga_6m, array(0.5))')[0].alias('cashout_telerecarga_6m'),\n",
    "# #     F.expr('percentile(cashout_automatic_debit_6m, array(0.5))')[0].alias('cashout_automatic_debit_6m'),\n",
    "# #     F.expr('percentile(cashout_withdraw_atm_6m, array(0.5))')[0].alias('cashout_withdraw_atm_6m'),\n",
    "# #     F.expr('percentile(cashout_rechazadas_con_ad_6m, array(0.5))')[0].alias('cashout_rechazadas_con_ad_6m'),\n",
    "# #     F.expr('percentile(cashout_rechazadas_sin_ad_6m, array(0.5))')[0].alias('cashout_rechazadas_sin_ad_6m')\n",
    "# #     )\n",
    "\n",
    "# # Frequency\n",
    "# df_freq = df_trx.select(['account_id', 'transaction_date']).filter((F.col('transaction_date') >= one_week_prior))\n",
    "# print(f'trx última semana: {df_freq.count()}')\n",
    "# df_freq = df_freq.groupby(['account_id', 'transaction_date']).count()\n",
    "# print(df_freq.show(2))\n",
    "# df_freq = df_freq.groupby(['account_id']).count().toDF(['account_id', 'f_1week'])\n",
    "# print(df_freq.show(2))\n",
    "\n",
    "\n",
    "\n",
    "# #####################\n",
    "# ### Merge todo ######\n",
    "# #####################\n",
    "# print('Mergear todos los df')\n",
    "# df_inferencias = (df_usr\n",
    "#                   .join(df_trx_6m, on='account_id', how='left')\n",
    "#                   .join(a, on='accountgp', how='left', suffixes=('_6m', '_1m'))\n",
    "#                   .merge(df_cashout_6m, on='accountgp', how='left')\n",
    "#                   .merge(df_cashout_1m, on='accountgp', how='left', suffixes=('_6m', '_1m'))\n",
    "#                   .merge(df_churn, on='accountgp', how='left')\n",
    "#                   )\n",
    "\n",
    "# df_inferencias.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# #####################\n",
    "# ##### Dummies #######\n",
    "# #####################\n",
    "\n",
    "# print('Generar dummies para \"sexo\" y \"provincia\"')\n",
    "# df_inferencias[\"sexo_\"] = df_inferencias[\"sexo\"] #mantener original\n",
    "# df_inferencias[\"sexo_\"] = df_inferencias[\"sexo_\"].astype('category')\n",
    "# df_inferencias[\"provincia_\"] = df_inferencias[\"provincia\"] #mantener original\n",
    "# df_inferencias[\"provincia_\"] = df_inferencias[\"provincia_\"].astype('category')\n",
    "# df_inferencias = pd.get_dummies(df_inferencias, columns=['sexo_', 'provincia_'], prefix_sep='', drop_first=True)\n",
    "\n",
    "# #####################\n",
    "# #  Inferencia coef  #\n",
    "# #####################\n",
    "# print('Inferencia')\n",
    "\n",
    "# coeficientes = ssm.get_parameter(Name='/test_adquirencia_coeficientes')['Parameter']['Value']\n",
    "# #coef = ast.literal_eval(str_)\n",
    "# coeficientes = json.loads(coeficientes)\n",
    "\n",
    "# coef_s = pd.Series(coeficientes).astype(float) # Vienen como str del parameter store\n",
    "# cols_coef = coef_s.drop('b0').index.tolist() # lista con las columnas que llevan coeficiente\n",
    "\n",
    "# df_inferencias['pred'] = (df_inferencias[cols_coef] # Nos quedamos solo con las columnas con coeficientes\n",
    "#                            .dot(coef_s[cols_coef]) # Multiplicación matricial con los coeficientes\n",
    "#                            .add(coef_s['b0']) # Sumamos b0 que no forma parte de la lista cols_coefi\n",
    "#                           )\n",
    "\n",
    "# #####################\n",
    "# ##### Save s3 #######\n",
    "# #####################\n",
    "# print('Guardar df final en s3')\n",
    "# wr.s3.to_parquet(\n",
    "#      df=df_inferencias,\n",
    "#      path=(f's3://{adquirencia_bucket}/{adquirencia_key_inf}/dt={now.strftime(\"%Y-%m-%d\")}/df_inference_{now.strftime(\"%Y-%m-%d\")}.parquet')\n",
    "#      #partition_efls=['col2']\n",
    "# )\n",
    "\n",
    "\n",
    "# ######################\n",
    "# # Actualizar LAST_RUN #\n",
    "# ######################\n",
    "# print('Actualizar last run')\n",
    "# ssm.put_parameter(\n",
    "#          Name=last_run_ps_name,\n",
    "#          Value=now.strftime(\"%Y-%m-%d\"),\n",
    "#          Type='String',\n",
    "#          Overwrite=True)\n",
    "# print('Last run actualizado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473c479e",
   "metadata": {},
   "source": [
    "# Subir archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ac6589b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".py uploaded\n"
     ]
    }
   ],
   "source": [
    "adquirencia_bucket   = 'test-datascience-adquirencia-fraude'\n",
    "# Guardar el archivo .py\n",
    "s3.meta.client.upload_file('job_adquirencia.py', \n",
    "                            adquirencia_bucket, #bucket\n",
    "                           'artifacts/code/job_adquirencia.py')\n",
    "print('.py uploaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4f0e18",
   "metadata": {},
   "source": [
    "### Archivos auxiliares: \n",
    "Por ahora los subimos manualmente:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c927b453",
   "metadata": {},
   "source": [
    "s3.meta.client.upload_file('accountids_gp.parquet', \n",
    "                            BUCKET_ADQUIRENCIA, #bucket\n",
    "                           'data/auxiliar/accountids_gp.parquet')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4df645ab",
   "metadata": {},
   "source": [
    "s3.meta.client.upload_file('df_cotizacion.parquet', \n",
    "                            BUCKET_ADQUIRENCIA, #bucket\n",
    "                           'data/auxiliar/df_cotizacion.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d03274",
   "metadata": {},
   "source": [
    "# Job Glue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba618d1",
   "metadata": {},
   "source": [
    "Parámetros: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "927709ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'job_adquirencia_fraude'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b58d66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adquirencia_bucket   = 'test-datascience-adquirencia-fraude'\n",
    "adquirencia_key_inf  = 'inferences'\n",
    "datalake_prod_bucket = 'uala-arg-datalake-analytics-prod'\n",
    "transactions_key     = 'ar/tb_ar_core_transactions'\n",
    "gp1010_key           = 'ar/tb_ar_gp_t1010'\n",
    "cotizaciones_bucket  = 'test-datascience-adquirencia-fraude'\n",
    "cotizaciones_key     = 'data/auxiliar'\n",
    "accountid_gp_bucket  = 'test-datascience-adquirencia-fraude'\n",
    "accountid_gp_key     = 'data/auxiliar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2318e090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'JobName': 'job_adquirencia_fraude',\n",
       " 'ResponseMetadata': {'RequestId': 'aff1ca43-d750-4855-8dfa-2701579da877',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Fri, 16 Jul 2021 14:02:17 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '36',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'aff1ca43-d750-4855-8dfa-2701579da877'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# borrar job\n",
    "glue.delete_job(\n",
    "    JobName=job_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d07a345c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AccessDeniedException",
     "evalue": "An error occurred (AccessDeniedException) when calling the CreateJob operation: User: arn:aws:sts::465900123368:assumed-role/iam_r_uala_arg_datalake_stage_sagemaker/SageMaker is not authorized to perform: iam:PassRole on resource: arn:aws:iam::465900123368:role/iam_r_uala_arg_datalake_stage_glue",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAccessDeniedException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-a401bb17208c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                       DefaultArguments={\n\u001b[1;32m      7\u001b[0m                         '--additional-python-modules': 'pip,setuptools,pyarrow==2,awswrangler==2.8.0,numpy==1.19.1'},\n\u001b[0;32m----> 8\u001b[0;31m                       \u001b[0mMaxCapacity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                       )\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAccessDeniedException\u001b[0m: An error occurred (AccessDeniedException) when calling the CreateJob operation: User: arn:aws:sts::465900123368:assumed-role/iam_r_uala_arg_datalake_stage_sagemaker/SageMaker is not authorized to perform: iam:PassRole on resource: arn:aws:iam::465900123368:role/iam_r_uala_arg_datalake_stage_glue"
     ]
    }
   ],
   "source": [
    "job = glue.create_job(Name=job_name, \n",
    "                      GlueVersion='2.0',\n",
    "                      Role='iam_r_uala_arg_datalake_stage_glue',\n",
    "                      Command={'Name': 'glueetl',\n",
    "                               'ScriptLocation': f's3://{adquirencia_bucket}/artifacts/code/job_adquirencia.py'},\n",
    "                      DefaultArguments={\n",
    "                        '--additional-python-modules': 'pip,setuptools,pyarrow==2,awswrangler==2.8.0,numpy==1.19.1'},\n",
    "                      MaxCapacity=1\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3da6da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_run = glue.start_job_run(\n",
    "    JobName = job_name,\n",
    "    Arguments = {\n",
    "         '--adquirencia_bucket':   adquirencia_bucket,\n",
    "         '--adquirencia_key_inf':  adquirencia_key_inf,\n",
    "         '--datalake_prod_bucket': datalake_prod_bucket,\n",
    "         '--transactions_key':     transactions_key,\n",
    "         '--gp1010_key':           gp1010_key,\n",
    "         '--cotizaciones_bucket':  cotizaciones_bucket,\n",
    "         '--cotizaciones_key':     cotizaciones_key,\n",
    "         '--accountid_gp_bucket':  accountid_gp_bucket,\n",
    "         '--accountid_gp_key':     accountid_gp_key\n",
    "    } \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "255d2f03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(job_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "af43bea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job run: SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "MAX_WAIT_TIME=time.time() + 60*10 # 1 hour\n",
    "max_time = time.time() + MAX_WAIT_TIME\n",
    "while time.time() < max_time:\n",
    "    response=glue.get_job_run(JobName=job_name, RunId=job_run['JobRunId'])\n",
    "    status = response['JobRun']['JobRunState']\n",
    "    clear_output(wait=True)\n",
    "    print('Job run: {}'.format(status))\n",
    "    \n",
    "    if status == 'SUCCEEDED' or status == 'FAILED':\n",
    "        break\n",
    "        \n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abba5bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8d73d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d081bb45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e6d32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2826c1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_parquet(f's3://{adquirencia_bucket}/data/test/df_6m.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7c162e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_id</th>\n",
       "      <th>transaction_month</th>\n",
       "      <th>cashin_tdc_6m</th>\n",
       "      <th>cashin_to_user_6m</th>\n",
       "      <th>cashin_cvu_6m</th>\n",
       "      <th>cashin_investments_withdraw_6m</th>\n",
       "      <th>cashout_purchase_6m</th>\n",
       "      <th>cashout_cvu_6m</th>\n",
       "      <th>cashout_user_to_user_6m</th>\n",
       "      <th>cashout_telerecarga_6m</th>\n",
       "      <th>cashout_automatic_debit_6m</th>\n",
       "      <th>cashout_withdraw_atm_6m</th>\n",
       "      <th>cashout_rechazadas_con_ad_6m</th>\n",
       "      <th>cashout_rechazadas_sin_ad_6m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5364018</th>\n",
       "      <td>d3aee692-5fe2-43a5-a915-0c8890acd648</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.384499</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.467028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.597422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013302</th>\n",
       "      <td>755ec547-9792-478e-83b5-6ba2424f2fe2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5911028</th>\n",
       "      <td>469d0097-22e0-49c5-a42a-4357b009ac05</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106.482346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103.410611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.758733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4617969</th>\n",
       "      <td>b396d35f-8deb-4369-9227-484bf998b8fe</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.217925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.427291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4745293</th>\n",
       "      <td>7e30becc-9a53-42a9-acfb-f78765d9dfcd</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.969481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   account_id transaction_month  \\\n",
       "5364018  d3aee692-5fe2-43a5-a915-0c8890acd648                 6   \n",
       "2013302  755ec547-9792-478e-83b5-6ba2424f2fe2                 6   \n",
       "5911028  469d0097-22e0-49c5-a42a-4357b009ac05                 6   \n",
       "4617969  b396d35f-8deb-4369-9227-484bf998b8fe                 5   \n",
       "4745293  7e30becc-9a53-42a9-acfb-f78765d9dfcd                 1   \n",
       "\n",
       "         cashin_tdc_6m  cashin_to_user_6m  cashin_cvu_6m  \\\n",
       "5364018            0.0                0.0       0.000000   \n",
       "2013302            0.0                0.0       0.000000   \n",
       "5911028            0.0                0.0     106.482346   \n",
       "4617969            0.0                0.0       0.000000   \n",
       "4745293            0.0                0.0       0.000000   \n",
       "\n",
       "         cashin_investments_withdraw_6m  cashout_purchase_6m  cashout_cvu_6m  \\\n",
       "5364018                             0.0             6.384499             0.0   \n",
       "2013302                             0.0             0.000000             0.0   \n",
       "5911028                             0.0           103.410611             0.0   \n",
       "4617969                             0.0             0.217925             0.0   \n",
       "4745293                             0.0             3.969481             0.0   \n",
       "\n",
       "         cashout_user_to_user_6m  cashout_telerecarga_6m  \\\n",
       "5364018                 0.467028                     0.0   \n",
       "2013302                 0.000000                     0.0   \n",
       "5911028                 0.000000                     0.0   \n",
       "4617969                 0.000000                     0.0   \n",
       "4745293                 0.000000                     0.0   \n",
       "\n",
       "         cashout_automatic_debit_6m  cashout_withdraw_atm_6m  \\\n",
       "5364018                    0.597422                      0.0   \n",
       "2013302                    0.000000                      0.0   \n",
       "5911028                    2.758733                      0.0   \n",
       "4617969                    0.000000                      0.0   \n",
       "4745293                    0.000000                      0.0   \n",
       "\n",
       "         cashout_rechazadas_con_ad_6m  cashout_rechazadas_sin_ad_6m  \n",
       "5364018                           0.0                      0.000000  \n",
       "2013302                           0.0                      0.000000  \n",
       "5911028                           0.0                      0.000000  \n",
       "4617969                           0.0                     25.427291  \n",
       "4745293                           0.0                      0.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "515902b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7082407, 14)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "345fce84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    1310577\n",
       "5    1301640\n",
       "4    1237987\n",
       "3    1124178\n",
       "2    1059513\n",
       "1    1048512\n",
       "Name: transaction_month, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.transaction_month.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4ab05a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.7 s, sys: 810 ms, total: 12.5 s\n",
      "Wall time: 12.6 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_id</th>\n",
       "      <th>cashin_tdc_6m</th>\n",
       "      <th>cashin_to_user_6m</th>\n",
       "      <th>cashin_cvu_6m</th>\n",
       "      <th>cashin_investments_withdraw_6m</th>\n",
       "      <th>cashout_purchase_6m</th>\n",
       "      <th>cashout_cvu_6m</th>\n",
       "      <th>cashout_user_to_user_6m</th>\n",
       "      <th>cashout_telerecarga_6m</th>\n",
       "      <th>cashout_automatic_debit_6m</th>\n",
       "      <th>cashout_withdraw_atm_6m</th>\n",
       "      <th>cashout_rechazadas_con_ad_6m</th>\n",
       "      <th>cashout_rechazadas_sin_ad_6m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000005b1-6bd9-473e-aecf-3c23739ef825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.781749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.123516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.370456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000750-eb3a-4dc1-a91e-701ac44aeec1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.364572</td>\n",
       "      <td>293.501876</td>\n",
       "      <td>268.083426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>268.083426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000f06-ce69-4024-96af-3d44f9fc7853</td>\n",
       "      <td>4.670278</td>\n",
       "      <td>0.093406</td>\n",
       "      <td>0.186811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.186811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00002479-2ed2-4c6d-9cc1-2e827857bf3d</td>\n",
       "      <td>3.026940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.025431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000024d1-43e8-4e26-b01f-22aa7759e678</td>\n",
       "      <td>2.394407</td>\n",
       "      <td>6.085457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.582019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.648693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884166</th>\n",
       "      <td>ffffe851-156f-4029-b3e4-6dc09fecffec</td>\n",
       "      <td>126.768868</td>\n",
       "      <td>100.897992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.670597</td>\n",
       "      <td>177.672956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884167</th>\n",
       "      <td>ffffeb20-8b5b-44e9-bdb1-e124ee152a3a</td>\n",
       "      <td>51.939845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.340557</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.457651</td>\n",
       "      <td>13.590510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.670278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884168</th>\n",
       "      <td>ffffebcd-a142-4cc6-a922-c444dc5b1f8e</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.262996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.505420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884169</th>\n",
       "      <td>ffffefc0-1686-4997-9a36-f8ec6a0f3e1b</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.202881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884170</th>\n",
       "      <td>fffffb43-ff6b-4b1b-bae0-11e8326255ed</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.580807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1884171 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   account_id  cashin_tdc_6m  \\\n",
       "0        000005b1-6bd9-473e-aecf-3c23739ef825       0.000000   \n",
       "1        00000750-eb3a-4dc1-a91e-701ac44aeec1       0.000000   \n",
       "2        00000f06-ce69-4024-96af-3d44f9fc7853       4.670278   \n",
       "3        00002479-2ed2-4c6d-9cc1-2e827857bf3d       3.026940   \n",
       "4        000024d1-43e8-4e26-b01f-22aa7759e678       2.394407   \n",
       "...                                       ...            ...   \n",
       "1884166  ffffe851-156f-4029-b3e4-6dc09fecffec     126.768868   \n",
       "1884167  ffffeb20-8b5b-44e9-bdb1-e124ee152a3a      51.939845   \n",
       "1884168  ffffebcd-a142-4cc6-a922-c444dc5b1f8e       0.000000   \n",
       "1884169  ffffefc0-1686-4997-9a36-f8ec6a0f3e1b       0.000000   \n",
       "1884170  fffffb43-ff6b-4b1b-bae0-11e8326255ed       0.000000   \n",
       "\n",
       "         cashin_to_user_6m  cashin_cvu_6m  cashin_investments_withdraw_6m  \\\n",
       "0                 0.000000      10.781749                        0.000000   \n",
       "1                24.364572     293.501876                      268.083426   \n",
       "2                 0.093406       0.186811                        0.000000   \n",
       "3                 0.000000       0.000000                        0.000000   \n",
       "4                 6.085457       0.000000                        0.000000   \n",
       "...                    ...            ...                             ...   \n",
       "1884166         100.897992       0.000000                        0.000000   \n",
       "1884167           0.000000       9.340557                        0.000000   \n",
       "1884168          11.262996       0.000000                        0.000000   \n",
       "1884169           0.000000       0.000000                        0.000000   \n",
       "1884170           0.000000       2.580807                        0.000000   \n",
       "\n",
       "         cashout_purchase_6m  cashout_cvu_6m  cashout_user_to_user_6m  \\\n",
       "0                   9.123516        0.000000                 0.000000   \n",
       "1                   0.000000      268.083426                 0.000000   \n",
       "2                   0.000000        0.000000                 0.186811   \n",
       "3                   0.000000        1.025431                 0.000000   \n",
       "4                   2.582019        0.000000                 1.648693   \n",
       "...                      ...             ...                      ...   \n",
       "1884166             1.670597      177.672956                 0.000000   \n",
       "1884167            47.457651       13.590510                 0.000000   \n",
       "1884168             0.000000        1.505420                 0.000000   \n",
       "1884169             0.000000        0.000000                 0.000000   \n",
       "1884170             0.000000        0.000000                 0.000000   \n",
       "\n",
       "         cashout_telerecarga_6m  cashout_automatic_debit_6m  \\\n",
       "0                           0.0                         0.0   \n",
       "1                           0.0                         0.0   \n",
       "2                           0.0                         0.0   \n",
       "3                           0.0                         0.0   \n",
       "4                           0.0                         0.0   \n",
       "...                         ...                         ...   \n",
       "1884166                     0.0                         0.0   \n",
       "1884167                     0.0                         0.0   \n",
       "1884168                     0.0                         0.0   \n",
       "1884169                     0.0                         0.0   \n",
       "1884170                     0.0                         0.0   \n",
       "\n",
       "         cashout_withdraw_atm_6m  cashout_rechazadas_con_ad_6m  \\\n",
       "0                            0.0                           0.0   \n",
       "1                            0.0                           0.0   \n",
       "2                            0.0                           0.0   \n",
       "3                            0.0                           0.0   \n",
       "4                            0.0                           0.0   \n",
       "...                          ...                           ...   \n",
       "1884166                      0.0                           0.0   \n",
       "1884167                      0.0                           0.0   \n",
       "1884168                      0.0                           0.0   \n",
       "1884169                      0.0                           0.0   \n",
       "1884170                      0.0                           0.0   \n",
       "\n",
       "         cashout_rechazadas_sin_ad_6m  \n",
       "0                           18.370456  \n",
       "1                            0.000000  \n",
       "2                            0.000000  \n",
       "3                            0.000000  \n",
       "4                            0.000000  \n",
       "...                               ...  \n",
       "1884166                      0.000000  \n",
       "1884167                      4.670278  \n",
       "1884168                      0.000000  \n",
       "1884169                      0.202881  \n",
       "1884170                      0.000000  \n",
       "\n",
       "[1884171 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test.drop('transaction_month',axis=1).groupby('account_id',as_index=False).median()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
